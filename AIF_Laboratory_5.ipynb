{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIF_Laboratory_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iStct0cYPCe7"
      },
      "source": [
        "# Neural Nets\n",
        "\n",
        "In this part of the laboratory, you are to complete the API for a Neural Net. Then afterwards, you are to construct various neural nets using the API to solve abstract learning problems.\n",
        "\n",
        "## Completing the implementation\n",
        "\n",
        "We have provided you a skeleton of the Neural Net in the Neural net code. You are to complete the unimplemented methods.\n",
        "\n",
        "The three classes `Input`, `PerformanceElem`, and `Neuron` all have incomplete implementation of the following two functions:\n",
        "     \n",
        "    def output(self)\n",
        "    def dOutdX(self, elem)\n",
        "Your first task is to fill in all 6 functions to complete the API.\n",
        "\n",
        "## Output\n",
        "\n",
        "The function `output(self)` produces the output of each of these elements.\n",
        "\n",
        "Be sure to use the sigmoid and ds/dz functions as discussed in the course:\n",
        "\n",
        "    o = s(z) = 1.0 / (1.0 + e**(-z))\n",
        "    ds(o)/dz = s(z) * (1 - s(z)) = o * (1 - o)\n",
        "and the performance function and its derivative as discussed in the course:\n",
        "\n",
        "    P(o) = -0.5 (d - o)**2\n",
        "    dP(o)/dx = (d - o)\n",
        "    \n",
        "## Derivatives\n",
        "\n",
        "The function `dOutdX(self, elem)` generates the value of the partial derivative with respect to a given weight element.\n",
        "\n",
        "Recall, neural nets update a given weight by computing the partial derivative of the performance function with respect to that weight. The formula we have used in class is as follows:\n",
        "\n",
        "    wi' = wi + rate * dP / dwi\n",
        "    \n",
        "In our code this is represented as (see `def train()` -- you don't have to implement this):\n",
        "\n",
        "    w.set_next_value( w.get_value() + rate * network.performance.dOutdX(w) )\n",
        "The element passed to the `dOutdX` function is always a weight. Namely it is the weight that we are doing the partial over. Your job is to figure out how to define `dOutdX()` in terms of recursively calling `dOutdX()` or `output()` over the inputs and weights of a network element.\n",
        "\n",
        "For example, consider the Performance element P, `P.dOutdX(w)` could be defined in the following recursive fashion:\n",
        "\n",
        "    dP/d(w) = dP/do * do/dw # applying the chain rule\n",
        "            = (d-o) * o.dOutdX(w)\n",
        "Here `o` is the output of the Neuron that is directly connected to P.\n",
        "\n",
        "For Neuron units, there would be more cases to consider. Namely,\n",
        "\n",
        "\n",
        "1.   The termination case where the weight being differentiated over is one of the (direct) weights of the current neuron.\n",
        "2.   The recursive case where the weight is not one that is directly connected (but is a descendant weight).\n",
        "\n",
        "This implementation models the process of computing the chain of derivatives recursively. This top-down approach works from the output layer towards the input layers. This is in contrast to the more conventional approach (taught in the course) where you computed a per-node little-delta, and then recursively computed the weight updates bottom-up, from input towards output layers.\n",
        "\n",
        "## About the API Classes\n",
        "\n",
        "Most of the classes in the Neural Network inherit from the following two abstract classes:\n",
        "\n",
        "### `ValuedElement`\n",
        "\n",
        "This interface class allows an element to have a settable value. `Input` (e.g. i1, i2) and `Weight` (e.g. w1A, wAB) are subclasses of `ValueElement`.\n",
        "\n",
        "Elements that are subclassed all have these methods:\n",
        "\n",
        "*  `set_value(self,val)` - set the value of the element\n",
        "*  `get_value(self)` - get the value of the element\n",
        "*  `get_name(self)` - get the name of the element\n",
        "\n",
        "### `DifferentiableElement`\n",
        "\n",
        "This abstract class defines the interface for elements that have outputs and are involved in partial derivatives.\n",
        "\n",
        "* `output(self):` returns the output of this element\n",
        "* `dOutdX(self,elem):` returns the partial derivative with respect to another element\n",
        "\n",
        "`Inputs`, `Neurons`, and `PerformanceElem` are the three subclasses that implement `DifferentiableElement`. You will have to complete the interface for these classes.\n",
        "\n",
        "### `Weight(ValuedElement)`\n",
        "\n",
        "Represents update-able weights in the network. In addition to `ValueElement` functions are the following methods, which are used for the training algorithm (you will not need them in your implementation):\n",
        "\n",
        "* `set_next_value(self,val):` which sets the next weight value in `self.next_value`\n",
        "* `update(self):` which sets the current weight to the value stored in `self.next_value`\n",
        "\n",
        "### `Input(DifferentiableElement, ValuedElement)`\n",
        "\n",
        "Represents inputs to the network. These may represent variable inputs as well as fixed inputs (i.e. threshold inputs) that are always set to -1. `output()` of `Input` units should simply return the value they are set to during training or testing.\n",
        "\n",
        "**`dOutdX(self, elem)` of an `Input` unit should return 0, since there are no weights directly connected into inputs.**\n",
        "\n",
        "### `Neuron(DifferentiableElement)`\n",
        "\n",
        "Represents the actual neurons in the neural net. The definitions for `output` and `dOutdX` already contains some code. Namely, we've implemented a value caching mechanism to speed up the training / testing process. So instead of implementing output and `dOutdx` directly you should instead implement:\n",
        "\n",
        "* `compute_output(self):`\n",
        "* `compute_doutdx(self,elem):`\n",
        "\n",
        "### `PerformanceElem(DifferentiableElement)`\n",
        "\n",
        "Represents a Performance Element that allows you to set the desired output.\n",
        "\n",
        "* `set_desired` which sets `my_desired_val`\n",
        "\n",
        "To better understand back-propagation, you should take a look at the methods `train` and `test` in the Neural net code to see how everything is put together.\n",
        "\n",
        "## Unit Testing\n",
        "\n",
        "Once you've completed the missing functions, we have provided the Neural net tester code to help you unit test. You will need to pass the tests in this unit tester before you can move on to the next parts.\n",
        "\n",
        "To check if your implementation works, run:\n",
        "\n",
        "    neural_net_tester([\"simple\"])\n",
        "This makes sure that your code works and can learn basic functions such as AND and OR.\n",
        "\n",
        "## Building Neural Nets\n",
        "\n",
        "Once you have finished implementing the Neural Net API, you will be tasked to build three networks to learn various abstract data sets.\n",
        "\n",
        "Here is an example of how to construct a basic neural network:\n",
        "\n",
        "    def make_neural_net_basic():\n",
        "       \"\"\"Returns a 1 neuron network with 2 variable inputs, and 1 fixed input.\"\"\"\n",
        "       i0 = Input('i0',-1.0) # this input is immutable\n",
        "       i1 = Input('i1',0.0)\n",
        "       i2 = Input('i2',0.0)\n",
        "       \n",
        "       w1A = Weight('w1A',1)\n",
        "       w2A = Weight('w2A',1)\n",
        "       wA = Weight('wA', 1)\n",
        "       \n",
        "       # the inputs must be in the same order as their associated weights\n",
        "       A = Neuron('A', [i1,i2,i0], [w1A,w2A,wA])\n",
        "       P = PerformanceElem(A, 0.0)\n",
        "       \n",
        "       # Package all the components into a network\n",
        "       # First list the PerformanceElem P\n",
        "       # Then list all neurons afterwards\n",
        "       net = Network(P,[A])\n",
        "       \n",
        "       return net\n",
        "       \n",
        "## Naming conventions\n",
        "\n",
        "IMPORTANT: Be sure to use the following naming convention when creating elements for your networks:\n",
        "\n",
        "Inputs:\n",
        "\n",
        "* Format: `'i'` + input_number\n",
        "* Conventions:\n",
        "   * Start numbering from 1.\n",
        "   * Use the same `i0` for all the fixed -1 inputs.\n",
        "* Examples: `'i1'`, `i2`.\n",
        "\n",
        "Weights:\n",
        "* Format `'w' + from_identifier + to_identifier`\n",
        "* Examples:\n",
        "   * `w1A` for weight from Input `i1` to Neuron `A`.\n",
        "   * `wBC` for weight from Neuron `B` to Neuron `C`.\n",
        "   \n",
        "Neurons:\n",
        "\n",
        "* Format: `alphabet_letter`.\n",
        "* Convention: Assign neuron names in order of distance to the inputs.\n",
        "* Example: `A` is the neuron closest to the inputs, and on the left-most (or top-most) side of the net.\n",
        "* For ties, order neurons from left to right or top to bottom (depending on how you draw orient your network).\n",
        "\n",
        "## Building a 2-layer Neural Net\n",
        "\n",
        "Now use the Neural Net API you've just completed and tested to create a two layer network that looks like the following. \n",
        "\n",
        "![alt text](https://ai6034.mit.edu/wiki/images/NeuralNet.png)\n",
        "\n",
        "Fill your answer in the function stub:\n",
        "   \n",
        "    def make_neural_net_two_layer()\n",
        "in the Neural net code. \n",
        "\n",
        "Your 2-layer neural net should now be able to learn slightly harder datasets, such as the classic non-linearly separable examples such as NOT-EQUAL (XOR) and EQUAL.\n",
        "\n",
        "When initializing the weights of the network, you should use random weights. To get deterministic random initial weights so that tests are reproducible you should first seed the random number generator, and then generate the random weights.\n",
        "\n",
        "    seed_random()\n",
        "    \n",
        "    wt = random_weight()\n",
        "    ...use wt...\n",
        "    wt2 = random_weight()\n",
        "    ...use wt2...\n",
        "    \n",
        "Note: the function `random_weight()` in the Neural net code uses the python function `random.randrange(-1,2)` to compute initial weights. This function generates values: -1, 0, 1 (randomly). While this may seem like a mistake, what we've found empirically is that this actually performs better than using `random.uniform(-1, 1)`. Be our guest and play around with the `random_weight` function. You'll find that Neural Nets can be quite sensitive to initialization weight settings. (Recall what happens if you set all weights to the same value.)    \n",
        "\n",
        "To test your completed network, run:\n",
        "\n",
        "    neural_net_tester([\"two_layer\"])\n",
        "Your network should learn and classify all the datasets in the `harder_data_set` from the Neural net data code with 100% accuracy.\n",
        "\n",
        "## Designing For More Challenging Datasets\n",
        "\n",
        "Now it's your turn to design the network. We want to be able to classify more complex data sets.\n",
        "\n",
        "Specifically we want you to design a new network that should theoretically be able to learn and classify the following datasets:\n",
        "\n",
        "1. The letter-L.\n",
        "\n",
        "       4 + -\n",
        "       3 + -\n",
        "       2 + -\n",
        "       1 + - - - -\n",
        "       0 - + + + +\n",
        "         0 1 2 3 4\n",
        "2. This moat-like shape:\n",
        "\n",
        "       4 - - - - -\n",
        "       3 -       -\n",
        "       2 -   +   -\n",
        "       1 -       -\n",
        "       0 - - - - -\n",
        "         0 1 2 3 4\n",
        "3. This `patchy` shape:\n",
        "\n",
        "       4 - -   + +\n",
        "       3 - -   + +\n",
        "       2\n",
        "       1 + +   - -\n",
        "       0 + +   - -\n",
        "         0 1 2 3 4\n",
        "         \n",
        "We claim that a network architecture containing 5 neuron nodes or less can fully learn and classify all three shapes. In fact, we require it!\n",
        "\n",
        "Construct a new network in:\n",
        "\n",
        "    def make_neural_net_challenging()\n",
        "that can (theoretically) perfectly learn and classify all three datasets.\n",
        "\n",
        "To test your network on the first 2 of these shapes, run\n",
        "\n",
        "    neural_net_tester([\"challenging\"])\n",
        "To pass our tests, your network must get 100% accuracy within 10000 iterations.\n",
        "\n",
        "Now try your architecture on the third dataset, patchy. Run:\n",
        "\n",
        "    neural_net_tester([\"patchy\"])\n",
        "Depending on your architecture and your initial weights, your network may either easily learn patchy or get stuck in a local maximum. Does your network completely learn the dataset within 10000 iterations? If not, take a look at the weights output at the end of the 10000 iterations. Plot the weights in terms of a linear function on a 2D graph. Do the boundaries tell you why there might be a local maximum?\n",
        "\n",
        "## Manually Setting Weights\n",
        "\n",
        "You can have your network learn the dataset `patchy` perfectly and very quickly if the proper weights are set.\n",
        "\n",
        "You can use either of these strategies to determine the proper weights.\n",
        "1. You can experimentally determine the right weights by running your network until it perfectly learns the dataset. You will probably need to increase the max-iterations parameter, or playing around with different initial weight settings.\n",
        "2. You can try to solve for the weights analytically.\n",
        "\n",
        "In either case, we want you to find preset weights to the same network that you built in the last part. Your new weight-preset network should be able to learn the `patchy` problem with only 1000 additional iterations of training.\n",
        "\n",
        "After you've found the optimal weights, fill in:\n",
        "\n",
        "    def make_neural_net_with_weights()\n",
        "To test, run:\n",
        "    \n",
        "    neural_net_tester([\"weights\"])\n",
        "    \n",
        "If everything tests with an accuracy of 1.0, then you've completed the Neural Networks portion of the laboratory.\n",
        "\n",
        "# Boosting\n",
        "\n",
        "You're still trying to use AI to predict the votes of politicians. ID-Trees were great, but you've heard about these other magnificent learning algorithms like SVMs and Boosting. Boosting sounds easier to implement and had a pretty good reputation, so you decide to start with that.\n",
        "\n",
        "To make sure that you interpret the results without letting your political preconceptions get in the way, you dig up some old data to work with: in particular, the data from the 4th House of Representatives, which met from 1796 to 1797. (This is when the two-party system first emerged, with the two parties being designated \"Federalists\" and \"Republicans\".)\n",
        "\n",
        "You experiment with that data before going on to the 2007-2008 data, finding that Congressmen in 1796 were much more clear about what they were voting on than in 2008.\n",
        "\n",
        "The framework for a boosting classifier can be found in the Boost code. You need to finish coding it, and then use it to learn some classifiers and answer a few questions.\n",
        "\n",
        "## A (clever|cheap) trick\n",
        "\n",
        "The boosting code uses a trick that means it only has to try half the number of base classifiers.\n",
        "\n",
        "It turns out that AdaBoost does not really care which side of its base classifier is +1 and which side is -1. If you choose a classifier that is the opposite of the best classifier -- it returns -1 for most points that should be +1, and returns +1 for most points that should be -1, and therefore has a high error rate -- it works the same as if you had chosen the negation of that classifier, which is the best classifier.\n",
        "\n",
        "If the data reweighting step is implemented correctly, it will produce the same weights given a classifier or its opposite. Also, a classifier with a high error rate will end up with a *negative* alpha value, so that in the final \"vote\" of classifiers it will act like its opposite. So the important thing about a classifier isn't that its error rate is *low* -- it's that the error rate is *far from 1/2*.\n",
        "\n",
        "In the boosting code, we take advantage of this. We include only classifiers that output +1 for voting YES on a particular motion, or +1 for voting NO on a particular motion, and as the \"best classifier\" we choose the classifier whose error rate is *farthest from 1/2*. If the error rate is high, then the result will act like a classifier that outputs +1 for \"voting NO or abstaining\", or +1 for \"voting YES or abstaining\", respectively. This means we don't have to include these classifiers in the base classifier list, speeding up the algorithm by a factor of 2.\n",
        "\n",
        "## Completing the code\n",
        "\n",
        "Here are the parts that you need to complete:\n",
        "* In the `BoostClassifier` class, the `classify` method is also undefined. Define it so that you can use a trained `BoostClassifier` as a classifier, outputting +1 or -1 based on the weighted results of its base classifiers.\n",
        "* In the `BoostClassifier` class in the Boost code, the `update_weights` method is undefined. You need to define this method so that it changes the data weights in the way prescribed by the AdaBoost algorithm. Note: There are two ways of implementing this update; they happen to be mathematically equivalent.)\n",
        "* In the To be implemented code, the `most_misclassified` function is undefined. You will need to define it to answer the questions.\n",
        "\n",
        "**Remember to use the supplied `legislator_info(datum)` to output your list of the most-misclassified data points!**\n",
        "\n",
        "## Questions\n",
        "\n",
        "Answer the two questions `republican_newspaper_vote` and `republican_sunset_vote` in the To be implemeted code.\n",
        "\n",
        "When you are asked how a particular political party would vote on a particular motion, disregard the possibility of abstaining. If your classifier results indicate that the party *wouldn't* vote NO, consider that an indication that the party would vote YES.\n",
        "\n",
        "# Hints\n",
        "\n",
        "## Neural Nets\n",
        "\n",
        "If you are having problems with getting your network to convergence on certain problems, try the following:\n",
        "1. Order your weight initialization (i.e. calls to `random_weight()`) from the bottom-most weights to the top-most weights in you network. While this ordering theoretically irrelevant, we've found that this ordering worked well in practice (in conjunction with 1 above). NN are unfortunately quite sensitive to initial weight settings.\n",
        "2. Play around with the `seed_random` function to try different starting random seeds, although seeding the random function with 0 is what worked for us.\n",
        "3. If none of these work, try setting weights that are close to what you want in terms of the final expected solution.\n",
        "\n",
        "## FAQs\n",
        "\n",
        "**Q:** I can't figure out how to write `compute_doutdx` for Neurons.\n",
        "\n",
        "**A:** Here's a more step-by-step explanation of what this function does:\n",
        "\n",
        "So you have a neural net, and you're at a particular neuron, call it D. This neuron D has some inputs - for example, maybe it has an input y_B from neuron B with weight w_BD, and an input y_C from neuron C with weight w_CD, and a constant input (-1) with weight w_D. So its total input - let's call it \"z\" - is (y_B\\*w_BD + y_C\\*w_CD + -1\\*w_D). It's going to put that input z through its sigmoid function to get its output - let's call that y_D.\n",
        "\n",
        "Now, we want to compute the derivative of y_D with respect to some weight w. There are two cases.\n",
        "\n",
        "In the first case, w is one of the weights on something that goes directly into D - that is, w is w_BD or w_CD or w_D. In that case, we just do chain rule once, and we see that\n",
        "\n",
        "d(y_D)/d(w) = d(y_D)/dz * dz/dw.\n",
        "\n",
        "We know the derivative of the sigmoid function, so d(y_D)/dz is y_D\\*(1-y_D). And the derivative of z with respect to w is just whatever thing that weight w was being multiplied by - so for example, if w was w_BD, then dz/dw would be y_B.\n",
        "\n",
        "In the second case, w is not one of the weights going directly into D, it's probably something in an earlier layer, so we have to do some more chain rule to find dz/dw. We know z is (y_B\\*w_BD + y_C\\*w_CD + -1\\*w_D) (because we defined it to be that), so to find dz/dw, we can find the derivative of each of those terms:\n",
        "\n",
        "dz/dw = w_BD\\*(d(y_B)/dw) + w_CD\\*(d(y_C)/dw) + w_D\\*(d(-1)/dw).\n",
        "\n",
        "And how do we get the d(y_B)/dw etc? Well, y_B is the output of neuron B, so that's just `B.dOutdX(w)`. However, w might not have come from any path through B, so we only want to include the terms for which w is actually relevant. For that we can use the `isa_descendant_weight_of function`.\n",
        "\n",
        "**Q:** How do I figure out how the boost classifier thinks a Republican would vote on the newspapers thing?\n",
        "\n",
        "**A:** The classifiers are supposed to return +1 for Republican and -1 for anything else (there's a comment right above the definition of `BaseVoteClassifier` in the Boost code that says this). `boost_1796.classifiers` is a list of the weak classifiers and their corresponding weights in the trained classifier. You can look through those and find the newspaper one, and look at whether the weights on it are positive or negative.\n",
        "\n",
        "**Q:** How can I tell which input goes with which weight, in the `my_inputs` and `my_weights` lists in a Neuron?\n",
        "\n",
        "**A:** The two lists are in the same order - `my_weights[0]` is the weight for `my_inputs[0]`, etc.\n",
        "\n",
        "**Q:** I can't get my \"challenging\" neural net to correctly recognize all of the data sets. What can I do?\n",
        "\n",
        "**A:** Make sure you read the hints. You'll just need to play around with the initial weights.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMJx0c2jnkI0"
      },
      "source": [
        "## Neural net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLdORYx4ntE3"
      },
      "source": [
        "# - In this file we have an incomplete skeleton of\n",
        "# a neural network implementation.  Follow the instructions\n",
        "# and complete the NotImplemented methods below.\n",
        "#\n",
        "\n",
        "import math\n",
        "import random\n",
        "from functools import cmp_to_key\n",
        "\n",
        "class ValuedElement(object):\n",
        "    \"\"\"\n",
        "    This is an abstract class that all Network elements inherit from\n",
        "    \"\"\"\n",
        "    def __init__(self,name,val):\n",
        "        self.my_name = name\n",
        "        self.my_value = val\n",
        "\n",
        "    def set_value(self,val):\n",
        "        self.my_value = val\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.my_value\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.my_name\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"%s(%s)\" %(self.my_name, self.my_value)\n",
        "\n",
        "class DifferentiableElement(object):\n",
        "    \"\"\"\n",
        "    This is an abstract interface class implemented by all Network\n",
        "    parts that require some differentiable element.\n",
        "    \"\"\"\n",
        "    def output(self):\n",
        "        raise NotImplementedError(\"This is an abstract method\")\n",
        "\n",
        "    def dOutdX(self, elem):\n",
        "        raise NotImplementedError(\"This is an abstract method\")\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"clears any precalculated cached value\"\"\"\n",
        "        pass\n",
        "\n",
        "class Input(ValuedElement,DifferentiableElement):\n",
        "    \"\"\"\n",
        "    Representation of an Input into the network.\n",
        "    These may represent variable inputs as well as fixed inputs\n",
        "    (Thresholds) that are always set to -1.\n",
        "    \"\"\"\n",
        "    def __init__(self,name,val):\n",
        "        ValuedElement.__init__(self,name,val)\n",
        "        DifferentiableElement.__init__(self)\n",
        "\n",
        "    def output(self):\n",
        "        \"\"\"\n",
        "        Returns the output of this Input node.\n",
        "        \n",
        "        returns: number (float or int)\n",
        "        \"\"\"\n",
        "        return self.get_value();\n",
        "\n",
        "    def dOutdX(self, elem):\n",
        "        \"\"\"\n",
        "        Returns the derivative of this Input node with respect to \n",
        "        elem.\n",
        "\n",
        "        elem: an instance of Weight\n",
        "\n",
        "        returns: number (float or int)\n",
        "        \"\"\"\n",
        "        return 0;\n",
        "\n",
        "class Weight(ValuedElement):\n",
        "    \"\"\"\n",
        "    Representation of an weight into a Neural Unit.\n",
        "    \"\"\"\n",
        "    def __init__(self,name,val):\n",
        "        ValuedElement.__init__(self,name,val)\n",
        "        self.next_value = None\n",
        "\n",
        "    def set_next_value(self,val):\n",
        "        self.next_value = val\n",
        "\n",
        "    def update(self):\n",
        "        self.my_value = self.next_value\n",
        "\n",
        "class Neuron(DifferentiableElement):\n",
        "    \"\"\"\n",
        "    Representation of a single sigmoid Neural Unit.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, inputs, input_weights, use_cache=True):\n",
        "        assert len(inputs)==len(input_weights)\n",
        "        for i in range(len(inputs)):\n",
        "            assert isinstance(inputs[i],(Neuron,Input))\n",
        "            assert isinstance(input_weights[i],Weight)\n",
        "        DifferentiableElement.__init__(self)\n",
        "        self.my_name = name\n",
        "        self.my_inputs = inputs # list of Neuron or Input instances\n",
        "        self.my_weights = input_weights # list of Weight instances\n",
        "        self.use_cache = use_cache\n",
        "        self.clear_cache()\n",
        "        self.my_descendant_weights = None\n",
        "\n",
        "    def get_descendant_weights(self):\n",
        "        \"\"\"\n",
        "        Returns a mapping of the names of direct weights into this neuron,\n",
        "        to all descendant weights.\n",
        "        \"\"\"\n",
        "        if self.my_descendant_weights is None:\n",
        "            self.my_descendant_weights = {}\n",
        "            inputs = self.get_inputs()\n",
        "            weights = self.get_weights()\n",
        "            for i in range(len(weights)):\n",
        "                weight = weights[i]\n",
        "                weight_name = weight.get_name()\n",
        "                self.my_descendant_weights[weight_name] = set()\n",
        "                input = inputs[i]\n",
        "                if not isinstance(input, Input):\n",
        "                    descendants = input.get_descendant_weights()\n",
        "                    for name, s in descendants.items():\n",
        "                        st = self.my_descendant_weights[weight_name]\n",
        "                        st = st.union(s)\n",
        "                        st.add(name)\n",
        "                        self.my_descendant_weights[weight_name] = st\n",
        "\n",
        "        return self.my_descendant_weights\n",
        "\n",
        "    def isa_descendant_weight_of(self, target, weight):\n",
        "        \"\"\"\n",
        "        Checks if [target] is a indirect input weight into this Neuron\n",
        "        via the direct input weight [weight].\n",
        "        \"\"\"\n",
        "        weights = self.get_descendant_weights()\n",
        "        if weight.get_name() in weights:\n",
        "            return target.get_name() in weights[weight.get_name()]\n",
        "        else:\n",
        "            raise Exception(\"weight %s is not connect to this node: %s\"\n",
        "                            %(weight, self))\n",
        "\n",
        "    def has_weight(self, weight):\n",
        "        \"\"\"\n",
        "        Checks if [weight] is a direct input weight into this Neuron.\n",
        "        \"\"\"\n",
        "        weights = self.get_descendant_weights()\n",
        "        return weight.get_name() in self.get_descendant_weights()\n",
        "\n",
        "    def get_weight_nodes(self):\n",
        "        return self.my_weights\n",
        "\n",
        "    def clear_cache(self):\n",
        "        self.my_output = None\n",
        "        self.my_doutdx = {}\n",
        "\n",
        "    def output(self):\n",
        "        # Implement compute_output instead!!\n",
        "        if self.use_cache:\n",
        "            # caching optimization, saves previously computed dOutDx.\n",
        "            if self.my_output is None:\n",
        "                self.my_output = self.compute_output()\n",
        "            return self.my_output\n",
        "        return self.compute_output()\n",
        "\n",
        "    def compute_output(self):\n",
        "        \"\"\"\n",
        "        Returns the output of this Neuron node, using a sigmoid as\n",
        "        the threshold function.\n",
        "\n",
        "        returns: number (float or int)\n",
        "        \"\"\"\n",
        "        # sum of all inputs*weights which enter this function\n",
        "        out = 0\n",
        "\n",
        "        inputs = self.get_inputs()\n",
        "        weights = self.get_weights()\n",
        "\n",
        "        for i in range(len(inputs)):\n",
        "            out += inputs[i].output() * weights[i].get_value()\n",
        "\n",
        "        return ( 1.0 / (1.0 + math.exp(-out)) )\n",
        "\n",
        "    def dOutdX(self, elem):\n",
        "        # Implement compute_doutdx instead!!\n",
        "        if self.use_cache:\n",
        "            # caching optimization, saves previously computed dOutDx.\n",
        "            if elem not in self.my_doutdx:\n",
        "                self.my_doutdx[elem] = self.compute_doutdx(elem)\n",
        "            return self.my_doutdx[elem]\n",
        "        return self.compute_doutdx(elem)\n",
        "\n",
        "    def compute_doutdx(self, elem):\n",
        "        \"\"\"\n",
        "        Returns the derivative of this Neuron node, with respect to weight\n",
        "        elem, calling output() and/or dOutdX() recursively over the inputs.\n",
        "\n",
        "        elem: an instance of Weight\n",
        "\n",
        "        returns: number (float/int)\n",
        "        \"\"\"\n",
        "        \n",
        "        out = self.output()\n",
        "        # derivative of the threshold, sigmoid, function\n",
        "        sig_der = out * (1 - out)\n",
        "\n",
        "        if self.has_weight(elem):\n",
        "            index = self.my_weights.index(elem)\n",
        "            sigmoid = self.get_inputs()[index].output()\n",
        "            d = sig_der*sigmoid\n",
        "        else:\n",
        "            d = 0\n",
        "            for i in range(len(self.get_weights())):\n",
        "                weight = self.my_weights[i]\n",
        "                if self.isa_descendant_weight_of(elem, weight):\n",
        "                    deriv = self.get_inputs()[i].dOutdX(elem)\n",
        "                    d += weight.get_value() * deriv\n",
        "            d = d * sig_der\n",
        "        return d\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.my_weights\n",
        "\n",
        "    def get_inputs(self):\n",
        "        return self.my_inputs\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.my_name\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Neuron(%s)\" %(self.my_name)\n",
        "\n",
        "class PerformanceElem(DifferentiableElement):\n",
        "    \"\"\"\n",
        "    Representation of a performance computing output node.\n",
        "    This element contains methods for setting the\n",
        "    desired output (d) and also computing the final\n",
        "    performance P of the network.\n",
        "\n",
        "    This implementation assumes a single output.\n",
        "    \"\"\"\n",
        "    def __init__(self,input,desired_value):\n",
        "        assert isinstance(input,(Input,Neuron))\n",
        "        DifferentiableElement.__init__(self)\n",
        "        self.my_input = input\n",
        "        self.my_desired_val = desired_value\n",
        "\n",
        "    def output(self):\n",
        "        \"\"\"\n",
        "        Returns the output of this PerformanceElem node.\n",
        "        \n",
        "        returns: number (float/int)\n",
        "        \"\"\"\n",
        "        # P(o) = -0.5 (d - o)**2\n",
        "\n",
        "        P = -0.5 * (self.my_desired_val - self.my_input.output()) ** 2;\n",
        "\n",
        "        return P;\n",
        "\n",
        "    def dOutdX(self, elem):\n",
        "        \"\"\"\n",
        "        Returns the derivative of this PerformanceElem node with respect\n",
        "        to some weight, given by elem.\n",
        "\n",
        "        elem: an instance of Weight\n",
        "\n",
        "        returns: number (int/float)\n",
        "        \"\"\"\n",
        "        # dP/d(w) = dP/do * do/dw # applying the chain rule\n",
        "        # = (d-o) * o.dOutdX(w)\n",
        "        return ( self.my_desired_val - self.my_input.output() ) * self.my_input.dOutdX(elem)\n",
        "\n",
        "    def set_desired(self,new_desired):\n",
        "        self.my_desired_val = new_desired\n",
        "\n",
        "    def get_input(self):\n",
        "        return self.my_input\n",
        "\n",
        "def alphabetize(x,y):\n",
        "    if x.get_name()>y.get_name():\n",
        "        return 1\n",
        "    return -1\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self,performance_node,neurons):\n",
        "        self.inputs =  []\n",
        "        self.weights = []\n",
        "        self.performance = performance_node\n",
        "        self.output = performance_node.get_input()\n",
        "        self.neurons = neurons[:]\n",
        "        self.neurons.sort(key = cmp_to_key(alphabetize))\n",
        "        for neuron in self.neurons:\n",
        "            self.weights.extend(neuron.get_weights())\n",
        "            for i in neuron.get_inputs():\n",
        "                if isinstance(i,Input) and not i.get_name()=='i0' and not i in self.inputs:\n",
        "                    self.inputs.append(i)\n",
        "        self.weights.reverse()\n",
        "        self.weights = []\n",
        "        for n in self.neurons:\n",
        "            self.weights += n.get_weight_nodes()\n",
        "\n",
        "    def clear_cache(self):\n",
        "        for n in self.neurons:\n",
        "            n.clear_cache()\n",
        "\n",
        "def seed_random():\n",
        "    \"\"\"Seed the random number generator so that random\n",
        "    numbers are deterministically 'random'\"\"\"\n",
        "    random.seed(0)\n",
        "\n",
        "def random_weight():\n",
        "    \"\"\"Generate a deterministic random weight\"\"\"\n",
        "    # We found that random.randrange(-1,2) to work well emperically \n",
        "    # even though it produces randomly 3 integer values -1, 0, and 1.\n",
        "    return random.randrange(-1, 2)\n",
        "\n",
        "    # Uncomment the following if you want to try a uniform distribuiton \n",
        "    # of random numbers compare and see what the difference is.\n",
        "    # return random.uniform(-1, 1)\n",
        "\n",
        "def make_neural_net_basic():\n",
        "    \"\"\"\n",
        "    Constructs a 2-input, 1-output Network with a single neuron.\n",
        "    This network is used to test your network implementation\n",
        "    and a guide for constructing more complex networks.\n",
        "\n",
        "    Naming convention for each of the elements:\n",
        "\n",
        "    Input: 'i'+ input_number\n",
        "    Example: 'i1', 'i2', etc.\n",
        "    Conventions: Start numbering at 1.\n",
        "                 For the -1 inputs, use 'i0' for everything\n",
        "\n",
        "    Weight: 'w' + from_identifier + to_identifier\n",
        "    Examples: 'w1A' for weight from Input i1 to Neuron A\n",
        "              'wAB' for weight from Neuron A to Neuron B\n",
        "\n",
        "    Neuron: alphabet_letter\n",
        "    Convention: Order names by distance to the inputs.\n",
        "                If equal distant, then order them left to right.\n",
        "    Example:  'A' is the neuron closest to the inputs.\n",
        "\n",
        "    All names should be unique.\n",
        "    You must follow these conventions in order to pass all the tests.\n",
        "    \"\"\"\n",
        "    i0 = Input('i0', -1.0) # this input is immutable\n",
        "    i1 = Input('i1', 0.0)\n",
        "    i2 = Input('i2', 0.0)\n",
        "\n",
        "    w1A = Weight('w1A', 1)\n",
        "    w2A = Weight('w2A', 1)\n",
        "    wA  = Weight('wA', 1)\n",
        "\n",
        "    # Inputs must be in the same order as their associated weights\n",
        "    A = Neuron('A', [i1,i2,i0], [w1A,w2A,wA])\n",
        "    P = PerformanceElem(A, 0.0)\n",
        "\n",
        "    net = Network(P,[A])\n",
        "    return net\n",
        "\n",
        "def make_neural_net_two_layer():\n",
        "    \"\"\"\n",
        "    Create a 2-input, 1-output Network with three neurons.\n",
        "    There should be two neurons at the first level, each receiving both inputs\n",
        "    Both of the first level neurons should feed into the second layer neuron.\n",
        "\n",
        "    See 'make_neural_net_basic' for required naming convention for inputs,\n",
        "    weights, and neurons.\n",
        "    \"\"\"\n",
        "    i0 = Input('i0', -1.0)\n",
        "    i1 = Input('i1', 0.0)\n",
        "    i2 = Input('i2', 0.0)\n",
        "\n",
        "    seed_random()\n",
        "\n",
        "    wA = Weight('wA', random_weight() )\n",
        "    wB = Weight('wB', random_weight() )\n",
        "    w1A = Weight('w1A', random_weight() )\n",
        "    w1B = Weight('w1B', random_weight() )\n",
        "    w2A = Weight('w2A', random_weight() )\n",
        "    w2B = Weight('w2B', random_weight() )\n",
        "\n",
        "    wAC = Weight('wAC', random_weight() ) \n",
        "    wBC = Weight('wBC', random_weight() )\n",
        "    wC = Weight('wC', random_weight() )\n",
        "\n",
        "    A = Neuron('A', [i0,i1,i2], [wA,w1A,w2A])\n",
        "    B = Neuron('B', [i0,i1,i2], [wB,w1B,w2B])\n",
        "\n",
        "    C = Neuron('C', [i0,A,B], [wC,wAC,wBC])\n",
        "\n",
        "    P = PerformanceElem(C, 0.0)\n",
        "\n",
        "    net = Network(P, [A, B, C])\n",
        "    return net\n",
        "    # raise NotImplementedError(\"Implement me!\")\n",
        "\n",
        "def make_neural_net_challenging():\n",
        "    \"\"\"\n",
        "    Design a network that can in-theory solve all 3 problems described in\n",
        "    the lab instructions.  Your final network should contain\n",
        "    at most 5 neuron units.\n",
        "\n",
        "    See 'make_neural_net_basic' for required naming convention for inputs,\n",
        "    weights, and neurons.\n",
        "    \"\"\"\n",
        "    i0 = Input('i0', -1.0)\n",
        "    i1 = Input('i1', 0.0)\n",
        "    i2 = Input('i2', 0.0)\n",
        "\n",
        "    seed_random()\n",
        "    w1A = Weight('w1A', random_weight())\n",
        "    w1B = Weight('w1B', random_weight())\n",
        "    w1C = Weight('w1C', random_weight())\n",
        "    w2A = Weight('w2A', random_weight())\n",
        "    w2B = Weight('w2B', random_weight())\n",
        "    w2C = Weight('w2C', random_weight())\n",
        "\n",
        "    wA = Weight('wA', random_weight())\n",
        "    wB = Weight('wB', random_weight())\n",
        "    wC = Weight('wC', random_weight())\n",
        "    wD = Weight('wD', random_weight())\n",
        "\n",
        "    wAD = Weight('wAD', random_weight())\n",
        "    wBD = Weight('wBD', random_weight())\n",
        "    wCD = Weight('wCD', random_weight())\n",
        "\n",
        "    A = Neuron('A', [i0, i1, i2], [wA, w1A, w2A])\n",
        "    B = Neuron('B', [i0, i1, i2], [wB, w1B, w2B])\n",
        "    C = Neuron('C', [i0, i1, i2], [wC, w1C, w2C])\n",
        "    D = Neuron('D', [i0, A, B, C], [wD, wAD, wBD, wCD])\n",
        "\n",
        "    P = PerformanceElem(D, 0.0)\n",
        "\n",
        "    net = Network(P, [A, B, C, D])\n",
        "    return net\n",
        "\n",
        "    # raise NotImplementedError(\"Implement me!\")\n",
        "\n",
        "def make_neural_net_with_weights():\n",
        "    \"\"\"\n",
        "    In this method you are to use the network you designed earlier\n",
        "    and set pre-determined weights.  Your goal is to set the weights\n",
        "    to values that will allow the \"patchy\" problem to converge quickly.\n",
        "    Your output network should be able to learn the \"patchy\"\n",
        "    dataset within 1000 iterations of back-propagation.\n",
        "    \"\"\"\n",
        "    # You can preset weights for the network by completing\n",
        "    # and uncommenting the init_weights dictionary below.\n",
        "    #\n",
        "    \n",
        "    \n",
        "   \n",
        "    init_weights = { \n",
        "      'wA': 1,\n",
        "      'w1A': -1,\n",
        "      'w2A': 1,\n",
        "      'wB': -1,\n",
        "      'w1B': 1,\n",
        "      'w2B': -1,\n",
        "      'wC': 1,\n",
        "      'w1C': -1,\n",
        "      'w2C': 1,\n",
        "      'wD': -1,\n",
        "      'wAD': 1,\n",
        "      'wBD': -1,\n",
        "      'wCD': 1,\n",
        "    }\n",
        "\n",
        "\n",
        "    return make_net_with_init_weights_from_dict(make_neural_net_challenging,\n",
        "                                                init_weights)\n",
        "\n",
        "def make_net_with_init_weights_from_dict(net_fn,init_weights):\n",
        "    net = net_fn()\n",
        "    for w in net.weights:\n",
        "        w.set_value(init_weights[w.get_name()])\n",
        "    return net\n",
        "\n",
        "def make_net_with_init_weights_from_list(net_fn,init_weights):\n",
        "    net = net_fn()\n",
        "    for i in range(len(net.weights)):\n",
        "        net.weights[i].set_value(init_weights[i])\n",
        "    return net\n",
        "\n",
        "\n",
        "def abs_mean(values):\n",
        "    \"\"\"Compute the mean of the absolute values a set of numbers.\n",
        "    For computing the stopping condition for training neural nets\"\"\"\n",
        "    abs_vals = map(lambda x: abs(x), values)\n",
        "    total = sum(abs_vals)\n",
        "\n",
        "    #test\n",
        "    if(len(list(abs_vals)) != 0):\n",
        "      return total / float(len(list(abs_vals)))\n",
        "\n",
        "    return total\n",
        "\n",
        "def train(network,\n",
        "          data,      # training data\n",
        "          rate=1.0,  # learning rate\n",
        "          target_abs_mean_performance=0.0001,\n",
        "          max_iterations=10000,\n",
        "          verbose=False):\n",
        "    \"\"\"Run back-propagation training algorithm on a given network.\n",
        "    with training [data].   The training runs for [max_iterations]\n",
        "    or until [target_abs_mean_performance] is reached.\n",
        "    \"\"\"\n",
        "    iteration = 0\n",
        "    while iteration < max_iterations:\n",
        "        fully_trained = False\n",
        "        performances = []  # store performance on each data point\n",
        "        for datum in data:\n",
        "            # set network inputs\n",
        "            for i in range(len(network.inputs)):\n",
        "                network.inputs[i].set_value(datum[i])\n",
        "\n",
        "            # set network desired output\n",
        "            network.performance.set_desired(datum[-1])\n",
        "\n",
        "            # clear cached calculations\n",
        "            network.clear_cache()\n",
        "\n",
        "            # compute all the weight updates\n",
        "            for w in network.weights:\n",
        "                w.set_next_value(w.get_value() +\n",
        "                                 rate * network.performance.dOutdX(w))\n",
        "\n",
        "            # set the new weights\n",
        "            for w in network.weights:\n",
        "                w.update()\n",
        "\n",
        "            # save the performance value\n",
        "            performances.append(network.performance.output())\n",
        "\n",
        "            # clear cached calculations\n",
        "            network.clear_cache()\n",
        "\n",
        "        # compute the mean performance value\n",
        "        abs_mean_performance = abs_mean(performances)\n",
        "\n",
        "        if abs_mean_performance < target_abs_mean_performance:\n",
        "            if verbose:\n",
        "                print(\"iter %d: training complete.\\n\"\\\n",
        "                      \"mean-abs-performance threshold %s reached (%1.6f)\"\\\n",
        "                      %(iteration,\n",
        "                        target_abs_mean_performance,\n",
        "                        abs_mean_performance))\n",
        "            break\n",
        "\n",
        "        iteration += 1\n",
        "        if iteration % 1000 == 0 and verbose:\n",
        "            print(\"iter %d: mean-abs-performance = %1.6f\"\\\n",
        "                  %(iteration,\n",
        "                    abs_mean_performance))\n",
        "\n",
        "\n",
        "def test(network, data, verbose=False):\n",
        "    \"\"\"Test the neural net on some given data.\"\"\"\n",
        "    correct = 0\n",
        "    for datum in data:\n",
        "\n",
        "        for i in range(len(network.inputs)):\n",
        "            network.inputs[i].set_value(datum[i])\n",
        "\n",
        "        # clear cached calculations\n",
        "        network.clear_cache()\n",
        "        result = network.output.output()\n",
        "        network.clear_cache()\n",
        "\n",
        "        rounded_result = round(result)\n",
        "        if round(result)==datum[-1]:\n",
        "            correct+=1\n",
        "            if verbose:\n",
        "                print(\"test(%s) returned: %s => %s [%s]\" %(str(datum),\n",
        "                                                           str(result),\n",
        "                                                           rounded_result,\n",
        "                                                           \"correct\"))\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"test(%s) returned: %s => %s [%s]\" %(str(datum),\n",
        "                                                           str(result),\n",
        "                                                           rounded_result,\n",
        "                                                           \"wrong\"))\n",
        "\n",
        "    return float(correct)/len(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "569hIE5WoDHf"
      },
      "source": [
        "## Neural net data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNa_tZP-oFON"
      },
      "source": [
        "#\n",
        "# Training and Test Data used in the Neural net tester code.\n",
        "#\n",
        "\"\"\"\n",
        "1++\n",
        "0-+\n",
        " 01\n",
        "\"\"\"\n",
        "or_data = ((0,0,0),\n",
        "           (0,1,1),\n",
        "           (1,0,1),\n",
        "           (1,1,1),\n",
        "           (0.25,0,0),\n",
        "           (0,0.25,0))\n",
        "\n",
        "or_test_data = ((0.1,0.1,0),\n",
        "                (0.1,0.9,1),\n",
        "                (0.9,0.1,1),\n",
        "                (0.9,0.9,1))\n",
        "\"\"\"\n",
        "1-+\n",
        "0--\n",
        " 01\n",
        "\"\"\"\n",
        "and_data = ((0,0,0),\n",
        "            (0,1,0),\n",
        "            (1,0,0),\n",
        "            (1,1,1),\n",
        "            (0.75,1.0,1),\n",
        "            (1.0,0.75,1))\n",
        "\n",
        "and_test_data = ((0.1,0.1,0),\n",
        "                 (0.1,0.9,0),\n",
        "                 (0.9,0.1,0),\n",
        "                 (0.9,0.9,1))\n",
        "\n",
        "\"\"\"\n",
        "1-+\n",
        "0+-\n",
        " 01\n",
        "\"\"\"\n",
        "equal_data = ((0,0,1),\n",
        "              (0,1,0),\n",
        "              (1,0,0),\n",
        "              (1,1,1))\n",
        "\n",
        "equal_test_data = ((0.1,0.1,1),\n",
        "                   (0.1,0.9,0),\n",
        "                   (0.9,0.1,0),\n",
        "                   (0.9,0.9,1))\n",
        "\n",
        "\"\"\"\n",
        "1+-\n",
        "0-+\n",
        " 01\n",
        "\"\"\"\n",
        "neq_data = ((0,0,0),\n",
        "            (0,1,1),\n",
        "            (1,0,1),\n",
        "            (1,1,0))\n",
        "\n",
        "neq_test_data = ((0.1,0.1,0),\n",
        "                 (0.1,0.9,1),\n",
        "                 (0.9,0.1,1),\n",
        "                 (0.9,0.9,0))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3-++-\n",
        "2-++-\n",
        "1-++-\n",
        "0-++-\n",
        " 0123\n",
        "\"\"\"\n",
        "vert_band_data = ((0,0,0),\n",
        "                  (0,1,0),\n",
        "                  (0,2,0),\n",
        "                  (0,3,0),\n",
        "                  (1,0,1),\n",
        "                  (1,1,1),\n",
        "                  (1,2,1),\n",
        "                  (1,3,1),\n",
        "                  (2,0,1),\n",
        "                  (2,1,1),\n",
        "                  (2,2,1),\n",
        "                  (2,3,1),\n",
        "                  (3,0,0),\n",
        "                  (3,1,0),\n",
        "                  (3,2,0),\n",
        "                  (3,3,0))\n",
        "\n",
        "vert_band_test_data = ((0,    1, 0),\n",
        "                        (0,    2, 0),\n",
        "                        (0,  1.5, 0),\n",
        "\n",
        "                        (1.5,  2, 1),\n",
        "                        (1.5,  5, 1),\n",
        "                        (1.5,  1, 1),\n",
        "\n",
        "                        (3,    1, 0),\n",
        "                        (3,  1.5, 0),\n",
        "                        (3,    2, 0),\n",
        "\n",
        "                        (1,  1.5, 1),\n",
        "                        (1, -1.5, 1),\n",
        "                        (2,  1.5, 1),\n",
        "                        (2, -1.5, 1),\n",
        "\n",
        "                        (4,  0,   0),\n",
        "                        (4,  4,   0),\n",
        "                        (-1, 0,   0),\n",
        "                        (-1, 4,   0))\n",
        "\n",
        "\"\"\"\n",
        "3----\n",
        "2++++\n",
        "1++++\n",
        "0----\n",
        " 0123\n",
        "\"\"\"\n",
        "horiz_band_data = ((0,0,0),\n",
        "                   (0,1,1),\n",
        "                   (0,2,1),\n",
        "                   (0,3,0),\n",
        "                   (1,0,0),\n",
        "                   (1,1,1),\n",
        "                   (1,2,1),\n",
        "                   (1,3,0),\n",
        "                   (2,0,0),\n",
        "                   (2,1,1),\n",
        "                   (2,2,1),\n",
        "                   (2,3,0),\n",
        "                   (3,0,0),\n",
        "                   (3,1,1),\n",
        "                   (3,2,1),\n",
        "                   (3,3,0))\n",
        "\n",
        "horiz_band_test_data = ((1, 1.5, 1),\n",
        "                        (2, 1.5, 1),\n",
        "                        (3, 1.5, 1),\n",
        "                        (0, 1.5, 1),\n",
        "                        (4,   0, 0),\n",
        "                        (4,   4, 0),\n",
        "                        (-1,  0, 0),\n",
        "                        (-1,  4, 0))\n",
        "\n",
        "\"\"\"\n",
        "4--- +\n",
        "3-- +\n",
        "2- + -\n",
        "1 + --\n",
        "0+ ---\n",
        " 01234\n",
        "\"\"\"\n",
        "diag_band_data = ((0,0,1),\n",
        "                  (1,1,1),\n",
        "                  (2,2,1),\n",
        "                  (3,3,1),\n",
        "                  (4,4,1),\n",
        "                  (0,4,0),\n",
        "                  (4,0,0),\n",
        "                  (0,3,0),\n",
        "                  (3,0,0),\n",
        "                  (0,2,0),\n",
        "                  (2,0,0),\n",
        "                  (1,4,0),\n",
        "                  (4,1,0),\n",
        "                  (1,3,0),\n",
        "                  (3,1,0),\n",
        "                  (2,4,0),\n",
        "                  (4,2,0),\n",
        "                  )\n",
        "\n",
        "diag_band_test_data = ((-1,-1,1),\n",
        "                       (5,  5,1),\n",
        "                       (-2,-2,1),\n",
        "                       (6,  6,1),\n",
        "                       (3.5,3.5,1),\n",
        "                       (1.5,1.5,1),\n",
        "                       (4,  0,0),\n",
        "                       (0,  4,0))\n",
        "\n",
        "\"\"\"\n",
        "4+++ -\n",
        "3++ -\n",
        "2+ - +\n",
        "1 - ++\n",
        "0- +++\n",
        " 01234\n",
        "\"\"\"\n",
        "idiag_band_data = ((0,0,0),\n",
        "                   (1,1,0),\n",
        "                   (2,2,0),\n",
        "                   (3,3,0),\n",
        "                   (4,4,0),\n",
        "                   (0,4,1),\n",
        "                   (4,0,1),\n",
        "                   (0,3,1),\n",
        "                   (3,0,1),\n",
        "                   (0,2,1),\n",
        "                   (2,0,1),\n",
        "                   (1,4,1),\n",
        "                   (4,1,1),\n",
        "                   (1,3,1),\n",
        "                   (3,1,1),\n",
        "                   (2,4,1),\n",
        "                   (4,2,1),\n",
        "                   )\n",
        "\n",
        "idiag_band_test_data = ((-1,-1,0),\n",
        "                        (5,  5,0),\n",
        "                        (-2,-2,0),\n",
        "                        (6,  6,0),\n",
        "                        (3.5,3.5,0),\n",
        "                        (1.5,1.5,0),\n",
        "                        (4,  0,1),\n",
        "                        (0,  4,1))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "4-----\n",
        "3-   -\n",
        "2- + -\n",
        "1-   -\n",
        "0-----\n",
        " 01234\n",
        "\"\"\"\n",
        "moat_data = ((0,0,0),\n",
        "             (1,0,0),\n",
        "             (2,0,0),\n",
        "             (3,0,0),\n",
        "             (4,0,0),\n",
        "\n",
        "             (1,1,0),\n",
        "             (4,1,0),\n",
        "\n",
        "             (1,2,0),\n",
        "             (3,3,1),\n",
        "             (4,2,0),\n",
        "\n",
        "             (1,4,0),\n",
        "             (4,4,0),\n",
        "\n",
        "             (0,4,0),\n",
        "             (1,4,0),\n",
        "             (2,4,0),\n",
        "             (3,4,0),\n",
        "             (4,4,0),\n",
        "             )\n",
        "\n",
        "moat_test_data = moat_data\n",
        "\n",
        "\"\"\"\n",
        "4+-\n",
        "3+-\n",
        "2+-\n",
        "1+----\n",
        "0-++++\n",
        " 01234\n",
        "\"\"\"\n",
        "letter_l_data = ((0,0,0),\n",
        "                 (1,0,1),\n",
        "                 (2,0,1),\n",
        "                 (3,0,1),\n",
        "                 (4,0,1),\n",
        "\n",
        "                 (1,1,0),\n",
        "                 (2,1,0),\n",
        "                 (3,1,0),\n",
        "                 (4,1,0),\n",
        "\n",
        "                 (0,2,1),\n",
        "                 (1,2,0),\n",
        "\n",
        "                 (0,3,1),\n",
        "                 (1,3,0),\n",
        "\n",
        "                 (0,4,1),\n",
        "                 (1,4,0),\n",
        "                 )\n",
        "\n",
        "letter_l_test_data = letter_l_data\n",
        "\n",
        "\"\"\"\n",
        "4-- ++\n",
        "3-- ++\n",
        "2\n",
        "1++ --\n",
        "0++ --\n",
        " 01234\n",
        "\"\"\"\n",
        "patch_data = ((0,0,1),\n",
        "              (0,1,1),\n",
        "              (1,0,1),\n",
        "              (1,1,1),\n",
        "\n",
        "              (3,0,0),\n",
        "              (3,1,0),\n",
        "              (4,0,0),\n",
        "              (4,1,0),\n",
        "\n",
        "              (0,3,0),\n",
        "              (0,4,0),\n",
        "              (1,3,0),\n",
        "              (1,4,0),\n",
        "\n",
        "              (3,3,1),\n",
        "              (3,4,1),\n",
        "              (4,3,1),\n",
        "              (4,4,1)\n",
        "              )\n",
        "\n",
        "patch_test_data = patch_data\n",
        "\n",
        "simple_data_sets = [(\"OR\", or_data, or_test_data),\n",
        "                    (\"AND\", and_data, and_test_data)\n",
        "                    ]\n",
        "\n",
        "harder_data_sets = [(\"EQUAL\", equal_data, equal_test_data),\n",
        "                    (\"NOT_EQUAL\", neq_data, neq_test_data),\n",
        "                    (\"horizontal-bands\", horiz_band_data, horiz_band_test_data),\n",
        "                    (\"vertical-bands\", vert_band_data, vert_band_test_data),\n",
        "                    (\"diagonal-band\", diag_band_data, diag_band_test_data),\n",
        "                    (\"inverse-diagonal-band\", idiag_band_data,\n",
        "                     idiag_band_test_data)\n",
        "                    ]\n",
        "\n",
        "challenging_data_sets = [(\"moat\", moat_data, moat_test_data),\n",
        "                         (\"letter-l\", letter_l_data, letter_l_test_data),\n",
        "                         ]\n",
        "\n",
        "manual_weight_data_sets = [(\"patchy\", patch_data, patch_test_data)]\n",
        "\n",
        "\n",
        "all_data_sets = simple_data_sets + harder_data_sets + challenging_data_sets + \\\n",
        "                manual_weight_data_sets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eH21qW_oaCI"
      },
      "source": [
        "## Neural net tester"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtCpsFyodBk"
      },
      "source": [
        "#\n",
        "# Unit tester for Neural net code.\n",
        "#\n",
        "import sys\n",
        "\n",
        "def main(neural_net_func, data_sets, max_iterations=10000):\n",
        "    verbose = True\n",
        "    for name, training_data, test_data in data_sets:\n",
        "        print(\"-\"*40)\n",
        "        print(\"Training on %s data\" %(name))\n",
        "        nn = neural_net_func()\n",
        "        train(nn, training_data, max_iterations=max_iterations,\n",
        "              verbose=verbose)\n",
        "        print(\"Trained weights:\")\n",
        "        for w in nn.weights:\n",
        "            print(\"Weight '%s': %f\"%(w.get_name(),w.get_value()))\n",
        "        print(\"Testing on %s test-data\" %(name))\n",
        "        result = test(nn, test_data, verbose=verbose)\n",
        "        print(\"Accuracy: %f\"%(result))\n",
        "\n",
        "def neural_net_tester(arg=None):\n",
        "    test_names = [\"simple\"]\n",
        "    if arg is not None:\n",
        "        test_names = arg\n",
        "\n",
        "    for test_name in test_names:\n",
        "        if test_name == \"simple\":\n",
        "            # these test simple logical configurations\n",
        "            main(make_neural_net_basic,\n",
        "                 simple_data_sets)\n",
        "\n",
        "        elif test_name == \"two_layer\":\n",
        "            # these test cases are slightly harder\n",
        "            main(make_neural_net_two_layer,\n",
        "                 simple_data_sets + harder_data_sets)\n",
        "\n",
        "        elif test_name == \"challenging\":\n",
        "            # these tests require a more complex architecture.\n",
        "            main(make_neural_net_challenging, challenging_data_sets)\n",
        "\n",
        "        elif test_name == \"patchy\":\n",
        "            # patchy problem is slightly tricky\n",
        "            # unless your network gets the right weights.\n",
        "            # it can quickly get stuck in local maxima.\n",
        "            main(make_neural_net_challenging, manual_weight_data_sets)\n",
        "\n",
        "        elif test_name == \"weights\":\n",
        "            # if you set the 'right' weights for\n",
        "            # the patchy problem it can converge very quickly.\n",
        "            main(make_neural_net_with_weights, manual_weight_data_sets,100)\n",
        "        else:\n",
        "            print(\"unrecognized test name %s\" %(test_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4PaVblRtLFB"
      },
      "source": [
        "## Data reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBrMS4u3tNfN"
      },
      "source": [
        "\"\"\"\n",
        "A set of utility functions for reading in the data format used by Keith T.\n",
        "Poole on voteview.com.\n",
        "\n",
        "You can download additional data that will work with these functions, for\n",
        "any Congress going back to the 1st, on that site.\n",
        "\"\"\"\n",
        "import csv\n",
        "from copy import deepcopy\n",
        "\n",
        "def legislator_info(legislator):\n",
        "    district = ''\n",
        "    if legislator['district'] > 0: district = '-%s' % legislator['district']\n",
        "    return \"%s (%s%s)\" % (legislator['name'], legislator['state'], district)\n",
        "\n",
        "def vote_info(vote):\n",
        "    if not vote['name']: return vote['number']\n",
        "    return \"%s: %s\" % (vote['number'], vote['name'])\n",
        "\n",
        "def is_interesting(vote):\n",
        "    return (vote['name'] != '')\n",
        "\n",
        "def title_case(str):\n",
        "    chars = list(str)\n",
        "    chars[0] = chars[0].upper()\n",
        "    for i in range(1, len(chars)):\n",
        "        if chars[i-1] not in ' -': chars[i] = chars[i].lower()\n",
        "    return ''.join(chars)\n",
        "\n",
        "state_codes = {}\n",
        "f = open('states.dat')\n",
        "for line in f:\n",
        "    state_codes[int(line[0:2])] = title_case(line[6:].strip())\n",
        "f.close()\n",
        "\n",
        "party_codes = {}\n",
        "f = open('party3.dat')\n",
        "for line in f:\n",
        "    party_codes[int(line[2:6])] = line[8:].strip()\n",
        "f.close()\n",
        "\n",
        "def vote_meaning(n):\n",
        "    if n in [1, 2, 3]: return 1\n",
        "    elif n in [4, 5, 6]: return -1\n",
        "    else: return 0\n",
        "\n",
        "def read_congress_data(filename):\n",
        "    \"\"\"\n",
        "    Reads a database of Congressional information in the format that comes\n",
        "    from Keith T. Poole's voteview.com.\n",
        "    \"\"\"\n",
        "    f = open(filename)\n",
        "    legislators = []\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        person = {}\n",
        "        person['state'] = state_codes[int(line[8:10])]\n",
        "        person['district'] = int(line[10:12])\n",
        "        person['party'] = party_codes[int(line[19:23])]\n",
        "        name = line[25:36].strip()\n",
        "        person['name'] = title_case(name.replace(\"  \", \", \"))\n",
        "        person['votes'] = [vote_meaning(int(x)) for x in line[36:]]\n",
        "        legislators.append(person)\n",
        "    f.close()\n",
        "    return legislators\n",
        "\n",
        "def read_vote_data(filename):\n",
        "    \"\"\"\n",
        "    Reads a CSV file of data on the votes that were taken.\n",
        "    \"\"\"\n",
        "    f = open(filename, encoding='utf-8')\n",
        "    csv_reader = csv.reader(f)\n",
        "    votes = []\n",
        "    for row in csv_reader:\n",
        "        if row[0] == \"date\":\n",
        "            continue # first line with column headers\n",
        "        vote = {}\n",
        "        vote['date'] = row[0]\n",
        "        vote['id'] = str(row[2])\n",
        "        if vote['id'] == '':\n",
        "            vote['id'] = str(len(votes))\n",
        "        vote['number'] = row[3]\n",
        "        vote['motion'] = row[4]\n",
        "        vote['name'] = row[6]\n",
        "        vote['result'] = row[5]\n",
        "        votes.append(vote)\n",
        "    f.close()\n",
        "    return votes\n",
        "\n",
        "def limit_votes(legislators, votes, n):\n",
        "    indices = [i for i in range(len(legislators[0]['votes'])-1, -1, -1) if\n",
        "    is_interesting(votes[i])][:n]\n",
        "\n",
        "    newleg = []\n",
        "    for leg in legislators:\n",
        "        leg = deepcopy(leg)\n",
        "        leg['votes'] = [leg['votes'][i] for i in indices]\n",
        "        found_any_votes = False\n",
        "        for vote in leg['votes']:\n",
        "            if vote != 0:\n",
        "                found_any_votes = True\n",
        "                break\n",
        "        if found_any_votes: newleg.append(leg)\n",
        "    newvotes = [votes[i] for i in indices]\n",
        "    return newleg, newvotes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr9fMhYNy4e3"
      },
      "source": [
        "## Boost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3fvor7Iy5xD"
      },
      "source": [
        "try:\n",
        "    set()\n",
        "except:\n",
        "    from sets import Set as set\n",
        "    \n",
        "import math\n",
        "\n",
        "class Classifier():\n",
        "    \"\"\"\n",
        "    An abstract class for classification. BaseClassifiers, StandardClassifiers\n",
        "    and BoostClassifiers inherit from this.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        raise NotImplementedError(\"This is an abstract class\")\n",
        "    def classify(self, obj):\n",
        "        raise NotImplementedError()\n",
        "    def error_rate(self, data, standard):\n",
        "        \"\"\"\n",
        "        Compare this classifier to a StandardClassifier and compute the\n",
        "        error rate.\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        total = len(data)\n",
        "        for datum in data:\n",
        "            if self.classify(datum) != standard.classify(datum): score += 1\n",
        "        return float(score)/total\n",
        "\n",
        "\n",
        "class StandardClassifier(Classifier):\n",
        "    \"\"\"\n",
        "    A classifier that returns the gold standard value.\n",
        "\n",
        "    In short, this classifier is one that, by designs, always returns the\n",
        "    \"right answer\". Its reason for existence is so that you can compare other\n",
        "    classifiers to it to test their accuracy.\n",
        "    \"\"\"\n",
        "    def __init__(self, key, value):\n",
        "        \"\"\"\n",
        "        Create a StandardClassifier.\n",
        "\n",
        "        key: the dictionary key or list index that stores an object's correct\n",
        "        class.\n",
        "\n",
        "        value: the class value that will be represented as +1. All other values\n",
        "        will be represented as -1.\n",
        "        \"\"\"\n",
        "        self.key = key\n",
        "        self.value = value\n",
        "\n",
        "    def classify(self, obj):\n",
        "        \"\"\"\n",
        "        If this object's class matches the given value, return +1. Otherwise,\n",
        "        return -1.\n",
        "        \"\"\"\n",
        "        if obj[self.key] == self.value: return 1\n",
        "        else: return -1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"<StandardClassifier: %s=%s>\" % (self.key, self.value)\n",
        "\n",
        "# The gold standard classifier we will use throughout is\n",
        "# standardPartyClassifier, which returns +1 for Republican or -1 for any other\n",
        "# party. The other party might be Democrat, Independent, or (for the 1796 data\n",
        "# set) Federalist.\n",
        "\n",
        "standardPartyClassifier = StandardClassifier('party', 'Republican')\n",
        "\n",
        "\n",
        "class BaseVoteClassifier(Classifier):\n",
        "    \"\"\"\n",
        "    A simplistic classifier that classifies a legislator based on a single\n",
        "    vote.\n",
        "    \"\"\"\n",
        "    def __init__(self, index, value, votelist):\n",
        "        \"\"\"\n",
        "        Make a base classifier that classifies a dictionary representing\n",
        "        a legislator, based on one of their votes.\n",
        "\n",
        "        index: the vote number to look up.\n",
        "        value: the value to check for. The classifier will return 1 if it\n",
        "               matches, and -1 if it doesn't.\n",
        "        votelist: a list of dictionaries that the classifier can use to\n",
        "                  look up the meanings of the votes.\n",
        "        \"\"\"\n",
        "        self.index = index\n",
        "        self.value = value\n",
        "        self.votelist = votelist\n",
        "\n",
        "    def classify(self, legislator):\n",
        "        if legislator['votes'][self.index] == self.value: return 1\n",
        "        else: return -1\n",
        "\n",
        "    def __str__(self):\n",
        "        vote = vote_info(self.votelist[self.index])\n",
        "        if self.value == 1: direction = 'YES'\n",
        "        else: direction = 'NO'\n",
        "        return \"%s on %s\" % (direction, vote)\n",
        "    def __repr__(self):\n",
        "        return \"<BaseVoteClassifier: %s>\" % self\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return (float(1)/(1+math.exp(-x)))\n",
        "\n",
        "def error_to_alpha(error_rate):\n",
        "    \"\"\"\n",
        "    Given an error rate, convert it to an alpha value -- that is, a weight to\n",
        "    assign to a base classifier. Low error rates get high alpha values.\n",
        "    \"\"\"\n",
        "    doubt = 1e-4\n",
        "    error_rate = min(max(error_rate, doubt), 1-doubt)\n",
        "    return (math.log(1-error_rate) - math.log(error_rate)) * 0.5\n",
        "\n",
        "class BoostClassifier(Classifier):\n",
        "    \"\"\"\n",
        "    A classifier that learns by composing several base classifiers using the\n",
        "    AdaBoost algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_classifiers, data, standard):\n",
        "        \"\"\"\n",
        "        Create a BoostClassifier.\n",
        "\n",
        "        base_classifiers: A list of all possible base classifiers to use. Note\n",
        "        that you automatically get the opposites of these classifiers as well.\n",
        "\n",
        "        data: the list of data points. These should be classifiable both by the\n",
        "        base classifiers and by the gold standard classifier.\n",
        "\n",
        "        standard: the \"gold standard\" classifier that always returns the\n",
        "        correct class for a data point.\n",
        "        \"\"\"\n",
        "        self.base_classifiers = base_classifiers\n",
        "        self.data = data \n",
        "        self.data_weights = [1.0/len(data) for d in data]\n",
        "        self.classifiers = []\n",
        "        self.standard = standard\n",
        "\n",
        "    def classify(self, obj):\n",
        "        \"\"\"\n",
        "        Once the boost classifier is trained, this function will use the\n",
        "        weighted combination of the base classifiers it learned to output a\n",
        "        final value.\n",
        "\n",
        "        It should return a class of 1 if the weighted total is positive, and -1\n",
        "        otherwise.\n",
        "\n",
        "        obj: a data point to classify\n",
        "\n",
        "        returns: int (+1 or -1)\n",
        "        \"\"\"\n",
        "        result = 0\n",
        "        for classifier, weight in self.classifiers:\n",
        "            result += classifier.classify(obj) * weight\n",
        "        if result >= 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def best_classifier(self):\n",
        "        \"\"\"\n",
        "        Returns the best base classifier for the current weights, along\n",
        "        with its error rate.\n",
        "\n",
        "        NOTE: the \"best\" classifier is the one that is the best at\n",
        "        distinguishing the two sets of data points. We're not requiring,\n",
        "        however, that it is the best at distinguishing them in the direction\n",
        "        you asked for. That is, a very high error rate like 0.999 is just as\n",
        "        good as a very low error rate like 0.001!\n",
        "\n",
        "        Why are we doing this? Because it essentially lets us invert any\n",
        "        base classifier. The mathematics of boosting means that it will\n",
        "        automatically count classifiers with error rates _above_ 0.5 as having\n",
        "        a negative weight, making them act just like their opposite classifier\n",
        "        which would have a low error rate.\n",
        "\n",
        "        As an example, this means that if one possible base classifier is\n",
        "        \"voted YES on legalizing ferrets\", this could also recognize a class\n",
        "        that \"voted NO or abstained on legalizing ferrets\" just by giving you\n",
        "        the YES classifier with a negative alpha (= a high error rate).\n",
        "        \"\"\"\n",
        "        best_error = 0.5\n",
        "        best_classifier = None\n",
        "        for classifier in set(self.base_classifiers) - set(self.classifiers):\n",
        "            error = 0.0\n",
        "            for datum, dweight in zip(self.data, self.data_weights):\n",
        "                if classifier.classify(datum) != self.standard.classify(datum):\n",
        "                    error += dweight\n",
        "            if abs(error-0.5) > abs(best_error-0.5):\n",
        "                best_error = error\n",
        "                best_classifier = classifier\n",
        "        if best_classifier is None:\n",
        "            # none of the classifiers work?\n",
        "            best_classifier = self.base_classifiers[0]\n",
        "        return (best_classifier, best_error)\n",
        "\n",
        "    def train(self, steps=10, verbose=False):\n",
        "        \"\"\"\n",
        "        Run the AdaBoost algorithm for a specified number of steps.\n",
        "        \"\"\"\n",
        "        for i in range(steps): self.step(verbose=verbose)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the classifier to its untrained state.\n",
        "        \"\"\"\n",
        "        self.data_weights = [1.0/len(self.data) for d in self.data]\n",
        "        self.classifiers = []\n",
        "\n",
        "    def renormalize_weights(self):\n",
        "        \"\"\"\n",
        "        Ensure that the weights of the data points add up to 1.\n",
        "\n",
        "        This should be called at every step. Even if your algorithm inherently\n",
        "        makes the points add up to 1, you may eventually need to correct\n",
        "        for floating-point drift.\n",
        "        \"\"\"\n",
        "        total = sum(self.data_weights)\n",
        "        self.data_weights = [w/total for w in self.data_weights]\n",
        "\n",
        "    def step(self, verbose=False):\n",
        "        \"\"\"\n",
        "        Run one step of boosting:\n",
        "\n",
        "        * Renormalize the weights\n",
        "        * Find the classifier that is best at distinguishing the classes\n",
        "          given these weights\n",
        "        * Update the weights based on the result of the classifier\n",
        "        \"\"\"\n",
        "        self.renormalize_weights()\n",
        "        best_classifier, best_error = self.best_classifier()\n",
        "        if verbose:\n",
        "            print(\"[error=%4.4f]\" % best_error, best_classifier)\n",
        "        self.update_weights(best_error, best_classifier)\n",
        "        self.classifiers.append((best_classifier, error_to_alpha(best_error)))\n",
        "\n",
        "    def update_weights(self, best_error, best_classifier):\n",
        "        \"\"\"\n",
        "        Follows the boosting algorithm to update the weights of the data points.\n",
        "\n",
        "        best_error: number (int/float) that is the error of the best classifier\n",
        "        best_classifier: Classifier instance which best classifies the data\n",
        "\n",
        "        returns: Nothing (only updates self.data_weights)\n",
        "        \"\"\"\n",
        "  #       [FORMULA]:\n",
        "  #  - [correctly classified point]: 1/2 * [ 1 / (1 - [error rate]) ] * [weight.old]\n",
        "  #  - [wrongly   classified point]: 1/2 * [ 1 /      [error rate]  ] * [weight.old]\n",
        "  # - This is done especially to highlight the [misclassified] points\n",
        "  # - We want [wrongly classified point] to have their [weight] increased,\n",
        "  #  so they are highlighted -> [correct points] have their [weight] lowered\n",
        "  \n",
        "        for i in range(len(self.data_weights)):\n",
        "            if best_classifier.classify(self.data[i]) == self.standard.classify(self.data[i]):\n",
        "                self.data_weights[i] = 0.5*(self.data_weights[i])/(1-best_error)\n",
        "            else:\n",
        "                self.data_weights[i] = 0.5*(self.data_weights[i])/best_error\n",
        "\n",
        "    def __str__(self):\n",
        "        classifier_part = '\\n'.join([\"%4.4f: %s\" % (weight, c) for c, weight in\n",
        "        self.classifiers])\n",
        "        return \"Boosting classifier:\\n\"+classifier_part\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"<Boosting classifier: %r>\" % self.classifiers\n",
        "\n",
        "def make_vote_classifiers(votelist):\n",
        "    \"\"\"\n",
        "    Given a list of votes, make two BaseVoteClassifiers for each vote\n",
        "    and return them all in a list.\n",
        "    \"\"\"\n",
        "    classifiers = []\n",
        "    for index in range(len(votelist)):\n",
        "        for value in (1, -1):\n",
        "            classifiers.append(BaseVoteClassifier(index, value, votelist))\n",
        "    return classifiers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REsh3Hp0zW_Y"
      },
      "source": [
        "## To be implemented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egoThwghzYvh"
      },
      "source": [
        "# Senate and House Data\n",
        "# These should be familiar by now.\n",
        "senate_people = read_congress_data('S110.ord')\n",
        "senate_votes = read_vote_data('S110desc.csv')\n",
        "\n",
        "house_people = read_congress_data('H110.ord')\n",
        "house_votes = read_vote_data('H110desc.csv')\n",
        "\n",
        "last_senate_people = read_congress_data('S109.ord')\n",
        "last_senate_votes = read_vote_data('S109desc.csv')\n",
        "\n",
        "house_1796 = read_congress_data('H004.ord')\n",
        "house_1796_votes = read_vote_data('H004desc.csv')\n",
        "\n",
        "\n",
        "# The first step is to complete the boosting code in boost.py. None of the\n",
        "# following steps will work without it.\n",
        "#\n",
        "# Once you've done that, the following line should make a boost classifier\n",
        "# that learns about the 4th House of Representatives (1795-1796).\n",
        "\n",
        "boost_1796 = BoostClassifier(make_vote_classifiers(house_1796_votes),\n",
        "                             house_1796, standardPartyClassifier)\n",
        "\n",
        "# You will need to train it, however. You can change the number of steps here.\n",
        "boost_1796.train(20)\n",
        "\n",
        "# Once you have run your boosting classifier for a sufficient number of steps\n",
        "# on the 4th House of Representatives data, it should tell you how it believes\n",
        "# Republicans and Federalists generally voted on a range of issues. Which way\n",
        "# does it predict a Republican would vote on the amendment to require\n",
        "# \"newspapers to be sufficiently dried before mailing\"? ('yes' or 'no')\n",
        "\n",
        "republican_newspaper_vote = 'no'\n",
        "\n",
        "# In the 4th House of Representatives, which five representatives were\n",
        "# misclassified the most while training your boost classifier?\n",
        "#\n",
        "# You should answer this question by defining the following function.\n",
        "# It should return five names of legislators, in the format that comes from\n",
        "# the legislator_info function. The tests will check the function, not just\n",
        "# its output in this case.\n",
        "\n",
        "def most_misclassified(classifier, n=5):\n",
        "    \"\"\"\n",
        "    Given a trained boosting classifier, return the n data points that were\n",
        "    misclassified the most (based on their final weights). The\n",
        "    most-misclassified datum should be at the beginning of the list.\n",
        "\n",
        "\tYou will need to use the \"legislator_info(datum)\" function to put your\n",
        "\toutput in the correct format.\n",
        "\n",
        "\tclassifier: instance of Classifier -- used to classify the data\n",
        "\tn: int -- the number of most-misclassified data points to return\n",
        "\n",
        "\treturns: list of data points (each passed through legislator_info) that were\n",
        "\t\t\t misclassified most often\n",
        "    \"\"\"\n",
        "   \n",
        "    # weights = [(classifier.data_weights[i], i) for i in range(len(classifier.data_weights))]\n",
        "    # output = []\n",
        "\n",
        "    # s = sorted(weights, key = lambda k : k[0])\n",
        "    # s.reverse()\n",
        "    # for i in range(n):\n",
        "    #     index = s[i][1]\n",
        "    #     output.append(legislator_info(classifier.data[index]))\n",
        "    # return output \n",
        "   \n",
        "\n",
        "    weights = [(classifier.data_weights[i], i) for i in range(len(classifier.data_weights))]\n",
        "\n",
        "    sortedWeights = sorted(weights, key=lambda weight : weight[0] )\n",
        "    sortedWeights.reverse()\n",
        "\n",
        "    output = [ legislator_info(classifier.data[index]) for index in [ x[1] for x in sortedWeights] ]\n",
        "    \n",
        "    final = output[:n]\n",
        "\n",
        "    return final\n",
        "\n",
        "# The following line is used by the tester; please leave it in place!\n",
        "most_misclassified_boost_1796 = lambda n: most_misclassified(boost_1796, n)\n",
        "\n",
        "# print(most_misclassified_boost_1796(5))\n",
        "\n",
        "# Now train a similar classifier on the 110th Senate (2007-2008).\n",
        "# How does it say a Republican would vote on Cardin Amdt No. 3930; To modify\n",
        "# the sunset provision (whatever that is)?\n",
        "\n",
        "boost = BoostClassifier(make_vote_classifiers(senate_votes), senate_people,\n",
        "  standardPartyClassifier)\n",
        "boost.train(20)\n",
        "republican_sunset_vote = 'no'\n",
        "\n",
        "# Which five Senators are the most misclassified after training your\n",
        "# classifier? (Again, the tester will test the function, not the answer you\n",
        "# print out here.)\n",
        "\n",
        "# The following line is used by the tester; please leave it in place!\n",
        "most_misclassified_boost = lambda n: most_misclassified(boost, n)\n",
        "\n",
        "# print(most_misclassified_boost(5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## The following code is used by the tester; please leave it in place!\n",
        "def classifier_tester(classifier_name, data_set):\n",
        "    \"\"\" Test a particular classifier, verify that it improves every step over 20 steps \"\"\"\n",
        "    return list(classifier_tester_helper(classifier_name, data_set))\n",
        "\n",
        "def classifier_tester_helper(classifier_name, data_set):\n",
        "    if classifier_name in globals():\n",
        "        classifier = globals()[classifier_name]\n",
        "        data = globals()[data_set]\n",
        "        if isinstance(classifier, Classifier):\n",
        "            original_classifier_count = len(classifier.classifiers)\n",
        "            classifier.reset()\n",
        "            for x in range(20):\n",
        "                classifier.step()\n",
        "                yield classifier.error_rate(data, standardPartyClassifier)\n",
        "\n",
        "            classifier.reset()\n",
        "            classifier.train(original_classifier_count)\n",
        "            return\n",
        "    raise Exception(\"Error: Classifier %s doesn't exist!, can't test it\" % classifier_name)\n",
        "\n",
        "def neural_net_tester(network_maker_func,\n",
        "                      train_dataset_name,\n",
        "                      test_dataset_name,\n",
        "                      iterations):\n",
        "    \"\"\"Test a neural net making function on a named dataset\"\"\"\n",
        "    seed_random()\n",
        "    network_maker_func = globals()[network_maker_func]\n",
        "    train_dataset = globals()[train_dataset_name]\n",
        "    test_dataset = globals()[test_dataset_name]\n",
        "    nn = network_maker_func()\n",
        "\n",
        "    train(nn, train_dataset, max_iterations=iterations)\n",
        "    result = test(nn, test_dataset)\n",
        "    return result\n",
        "\n",
        "def neural_net_size_tester(network_maker_func):\n",
        "    \"\"\"Test a neural net size\"\"\"\n",
        "    network_maker_func = globals()[network_maker_func]\n",
        "    nn = network_maker_func()\n",
        "    return len(nn.neurons)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr4IqJ8_0m-U"
      },
      "source": [
        "## Tester"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqQoKaNT0srX"
      },
      "source": [
        "from xmlrpc import client\n",
        "import traceback\n",
        "import sys\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "try:\n",
        "    from StringIO import StringIO\n",
        "except ImportError:\n",
        "    from io import StringIO\n",
        "\n",
        "\n",
        "# This is a skeleton for what the tester should do. Ideally, this module\n",
        "# would be imported in the pset and run as its main function.\n",
        "\n",
        "# We need the following rpc functions. (They generally take username and\n",
        "# password, but you could adjust this for whatever security system.)\n",
        "#\n",
        "# tester.submit_code(username, password, pset, studentcode)\n",
        "#   'pset' is a string such as 'ps0'. studentcode is a string containing\n",
        "#   the contents of the corresponding file, ps0.py. This stores the code on\n",
        "#   the server so we can check it later for cheating, and is a prerequisite\n",
        "#   to the tester returning a grade.\n",
        "#\n",
        "# tester.get_tests(pset)\n",
        "#   returns a list of tuples of the form (INDEX, TYPE, NAME, ARGS):\n",
        "#     INDEX is a unique integer that identifies the test.\n",
        "#     TYPE should be one of either 'VALUE' or 'FUNCTION'.\n",
        "#     If TYPE is 'VALUE', ARGS is ignored, and NAME is the name of a\n",
        "#     variable to return for this test.  The variable must be an attribute\n",
        "#     of the lab module.\n",
        "#     If TYPE is 'FUNCTION', NAME is the name of a function in the lab module\n",
        "#     whose return value should be the answer to this test, and ARGS is a\n",
        "#     tuple containing arguments for the function.\n",
        "#\n",
        "# tester.send_answer(username, password, pset, index, answer)\n",
        "#   Sends <answer> as the answer to test case <index> (0-numbered) in the pset\n",
        "#   named <pset>. Returns whether the answer was correct, and an expected\n",
        "#   value.\n",
        "#\n",
        "# tester.status(username, password, pset)\n",
        "#   A string that includes the official score for this user on this pset.\n",
        "#   If a part is missing (like the code), it should say so.\n",
        "\n",
        "# Because I haven't written anything on the server side, test_online has never\n",
        "# been tested.\n",
        "\n",
        "def test_summary(dispindex, ntests, testname):\n",
        "    return \"Test %d/%d (%s)\" % (dispindex, ntests, testname)\n",
        "\n",
        "tests = []\n",
        "\n",
        "def show_result(testsummary, testcode, correct, got, expected, verbosity):\n",
        "    \"\"\" Pretty-print test results \"\"\"\n",
        "    if correct:\n",
        "        if verbosity > 0:\n",
        "            print(\"%s: Correct.\" % testsummary)\n",
        "        if verbosity > 1:\n",
        "            print('\\t', testcode)\n",
        "            print('')\n",
        "    else:\n",
        "        print(\"%s: Incorrect.\" % testsummary)\n",
        "        print('\\t', testcode)\n",
        "        print(\"Got:     \", got)\n",
        "        print(\"Expected:\", expected)\n",
        "\n",
        "def show_exception(testsummary, testcode):\n",
        "    \"\"\" Pretty-print exceptions (including tracebacks) \"\"\"\n",
        "    print(\"%s: Error.\" % testsummary)\n",
        "    print(\"While running the following test case:\")\n",
        "    print('\\t', testcode)\n",
        "    print(\"Your code encountered the following error:\")\n",
        "    traceback.print_exc()\n",
        "    print('')\n",
        "\n",
        "\n",
        "def get_lab_module():\n",
        "    # Try the easy way first\n",
        "    try:\n",
        "        from tests import lab_number\n",
        "    except ImportError:\n",
        "        lab_number = None\n",
        "\n",
        "    if lab_number != None:\n",
        "        lab = __import__('lab%s' % lab_number)\n",
        "        return lab\n",
        "\n",
        "    lab = None\n",
        "\n",
        "    for labnum in range(10):\n",
        "        try:\n",
        "            lab = __import__('lab%s' % labnum)\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "    if lab == None:\n",
        "        raise ImportError(\"Cannot find your lab; or, error importing it.  Try loading it by running 'python labN.py' (for the appropriate value of 'N').\")\n",
        "\n",
        "    if not hasattr(lab, \"LAB_NUMBER\"):\n",
        "        lab.LAB_NUMBER = labnum\n",
        "\n",
        "    return lab\n",
        "\n",
        "def type_decode(arg, lab):\n",
        "    \"\"\"\n",
        "    XMLRPC can only pass a very limited collection of types.\n",
        "    Frequently, we want to pass a subclass of 'list' in as a test argument.\n",
        "    We do that by converting the sub-type into a regular list of the form:\n",
        "    [ 'TYPE', (data) ] (ie., AND(['x','y','z']) becomes ['AND','x','y','z']).\n",
        "    This function assumes that TYPE is a valid attr of 'lab' and that TYPE's\n",
        "    constructor takes a list as an argument; it uses that to reconstruct the\n",
        "    original data type.\n",
        "    \"\"\"\n",
        "    if isinstance(arg, list) and len(arg) >= 1: # We'll leave tuples reserved for some other future magic\n",
        "        try:\n",
        "            mytype = arg[0]\n",
        "            data = arg[1:]\n",
        "            return getattr(lab, mytype)([ type_decode(x, lab) for x in data ])\n",
        "        except AttributeError:\n",
        "            return [ type_decode(x, lab) for x in arg ]\n",
        "        except TypeError:\n",
        "            return [ type_decode(x, lab) for x in arg ]\n",
        "    else:\n",
        "        return arg\n",
        "\n",
        "\n",
        "def type_encode(arg):\n",
        "    \"\"\"\n",
        "    Encode trees as lists in a way that can be decoded by 'type_decode'\n",
        "    \"\"\"\n",
        "    if isinstance(arg, list) and not type(arg) in (list,tuple):\n",
        "        return [ arg.__class__.__name__ ] + [ type_encode(x) for x in arg ]\n",
        "    elif hasattr(arg, '__class__') and arg.__class__.__name__ == 'IF':\n",
        "        return [ 'IF', type_encode(arg._conditional), type_encode(arg._action), type_encode(arg._delete_clause) ]\n",
        "    else:\n",
        "        return arg\n",
        "\n",
        "\n",
        "def run_test(test, lab):\n",
        "    \"\"\"\n",
        "    Takes a 'test' tuple as provided by the online tester\n",
        "    (or generated by the offline tester) and executes that test,\n",
        "    returning whatever output is expected (the variable that's being\n",
        "    queried, the output of the function being called, etc)\n",
        "\n",
        "    'lab' (the argument) is the module containing the lab code.\n",
        "\n",
        "    'test' tuples are in the following format:\n",
        "      'id': A unique integer identifying the test\n",
        "      'type': One of 'VALUE', 'FUNCTION', 'MULTIFUNCTION', or 'FUNCTION_ENCODED_ARGS'\n",
        "      'attr_name': The name of the attribute in the 'lab' module\n",
        "      'args': a list of the arguments to be passed to the function; [] if no args.\n",
        "      For 'MULTIFUNCTION's, a list of lists of arguments to be passed in\n",
        "    \"\"\"\n",
        "    id, mytype, attr_name, args = test\n",
        "\n",
        "    attr = getattr(lab, attr_name)\n",
        "\n",
        "    if mytype == 'VALUE':\n",
        "        return attr\n",
        "    elif mytype == 'FUNCTION':\n",
        "        return apply(attr, args)\n",
        "    elif mytype == 'MULTIFUNCTION':\n",
        "        return [ run_test( (id, 'FUNCTION', attr_name, FN), lab) for FN in args ]\n",
        "    elif mytype == 'FUNCTION_ENCODED_ARGS':\n",
        "        return run_test( (id, 'FUNCTION', attr_name, type_decode(args, lab)), lab )\n",
        "    else:\n",
        "        raise Exception(\"Test Error: Unknown TYPE '%s'.  Please make sure you have downloaded the latest version of the tester script.  If you continue to see this error, contact a TA.\")\n",
        "\n",
        "\n",
        "def test_offline(verbosity=1):\n",
        "    \"\"\" Run the unit tests in 'tests.py' \"\"\"\n",
        "    import tests as tests_module\n",
        "\n",
        "#    tests = [ (x[:-8],\n",
        "#               getattr(tests_module, x),\n",
        "#               getattr(tests_module, \"%s_testanswer\" % x[:-8]),\n",
        "#               getattr(tests_module, \"%s_expected\" % x[:-8]),\n",
        "#               \"_\".join(x[:-8].split('_')[:-1]))\n",
        "#              for x in tests_module.__dict__.keys() if x[-8:] == \"_getargs\" ]\n",
        "\n",
        "#    tests = tests_module.get_tests()\n",
        "    global tests \n",
        "\n",
        "    ntests = len(tests)\n",
        "    ncorrect = 0\n",
        "\n",
        "    for index, (testname, getargs, testanswer, expected, fn_name, type) in enumerate(tests):\n",
        "        dispindex = index+1\n",
        "        summary = test_summary(dispindex, ntests, fn_name)\n",
        "\n",
        "        try:\n",
        "            if callable(getargs):\n",
        "                getargs = getargs()\n",
        "\n",
        "            if type == 'FUNCTION':\n",
        "                answer = fn_name(*getargs)\n",
        "            elif type == 'VALUE':\n",
        "                answer = fn_name\n",
        "            else:\n",
        "                answer = [ FN(*getargs) for FN in getargs ]#run_test((index, type, fn_name, getargs), get_lab_module())\n",
        "        except NotImplementedError:\n",
        "            print(\"%d: (%s: Function not yet implemented, NotImplementedError raised)\" % (index, testname))\n",
        "            continue\n",
        "        except Exception:\n",
        "            show_exception(summary, testname)\n",
        "            continue\n",
        "\n",
        "        correct = testanswer(answer, original_val = getargs)\n",
        "        show_result(summary, testname, correct, answer, expected, verbosity)\n",
        "        if correct: ncorrect += 1\n",
        "\n",
        "    print(\"Passed %d of %d tests.\" % (ncorrect, ntests))\n",
        "    tests = []\n",
        "    return (ncorrect == ntests)\n",
        "\n",
        "\n",
        "\n",
        "def get_target_upload_filedir():\n",
        "    \"\"\" Get, via user prompting, the directory containing the current lab \"\"\"\n",
        "    cwd = os.getcwd() # Get current directory.  Play nice with Unicode pathnames, just in case.\n",
        "\n",
        "    print(\"Please specify the directory containing your lab.\")\n",
        "    print(\"Note that all files from this directory will be uploaded!\")\n",
        "    print(\"Labs should not contain large amounts of data; very-large\")\n",
        "    print(\"files will fail to upload.\")\n",
        "    print('')\n",
        "    print(\"The default path is '%s'\" % cwd)\n",
        "    target_dir = raw_input(\"[%s] >>> \" % cwd)\n",
        "\n",
        "    target_dir = target_dir.strip()\n",
        "    if target_dir == '':\n",
        "        target_dir = cwd\n",
        "\n",
        "    print(\"Ok, using '%s'.\" % target_dir)\n",
        "\n",
        "    return target_dir\n",
        "\n",
        "def get_tarball_data(target_dir, filename):\n",
        "    \"\"\" Return a binary String containing the binary data for a tarball of the specified directory \"\"\"\n",
        "    data = StringIO()\n",
        "    file = tarfile.open(filename, \"w|bz2\", data)\n",
        "\n",
        "    print(\"Preparing the lab directory for transmission...\")\n",
        "\n",
        "    file.add(target_dir+\"/lab5.py\")\n",
        "    file.add(target_dir+\"/neural_net.py\")\n",
        "    file.add(target_dir+\"/boost.py\")\n",
        "    file.add(target_dir+\"/key.py\")\n",
        "\n",
        "    print(\"Done.\")\n",
        "    print('')\n",
        "    print(\"The following files have been added:\")\n",
        "\n",
        "    for f in file.getmembers():\n",
        "        print(f.name)\n",
        "\n",
        "    file.close()\n",
        "\n",
        "    return data.getvalue()\n",
        "\n",
        "\n",
        "def test_online(verbosity=1):\n",
        "    \"\"\" Run online unit tests.  Run them against the 6.034 server via XMLRPC. \"\"\"\n",
        "    lab = get_lab_module()\n",
        "\n",
        "    try:\n",
        "        server = xmlrpclib.Server(server_url, allow_none=True)\n",
        "        print(\"Getting tests:\", (username, password, lab.__name__))\n",
        "        tests = server.get_tests(username, password, lab.__name__)\n",
        "        print(\"*** TESTS:\")\n",
        "        print(tests)\n",
        "\n",
        "    except NotImplementedError: # Solaris Athena doesn't seem to support HTTPS\n",
        "        print(\"Your version of Python doesn't seem to support HTTPS, for\")\n",
        "        print(\"secure test submission.  Would you like to downgrade to HTTP?\")\n",
        "        print(\"(note that this could theoretically allow a hacker with access\")\n",
        "        print(\"to your local network to find your 6.034 password)\")\n",
        "        answer = raw_input(\"(Y/n) >>> \")\n",
        "        if len(answer) == 0 or answer[0] in \"Yy\":\n",
        "            server = xmlrpclib.Server(server_url.replace(\"https\", \"http\"))\n",
        "            tests = server.get_tests(username, password, lab.__name__)\n",
        "        else:\n",
        "            print(\"Ok, not running your tests.\")\n",
        "            print(\"Please try again on another computer.\")\n",
        "            print(\"Linux Athena computers are known to support HTTPS,\")\n",
        "            print(\"if you use the version of Python in the 'python' locker.\")\n",
        "            sys.exit(0)\n",
        "\n",
        "    ntests = len(tests)\n",
        "    ncorrect = 0\n",
        "\n",
        "    lab = get_lab_module()\n",
        "\n",
        "    target_dir = get_target_upload_filedir()\n",
        "\n",
        "    tarball_data = get_tarball_data(target_dir, \"lab%s.tar.bz2\" % lab.LAB_NUMBER)\n",
        "\n",
        "    print(\"Submitting to the 6.034 Webserver...\")\n",
        "\n",
        "    server.submit_code(username, password, lab.__name__, xmlrpclib.Binary(tarball_data))\n",
        "\n",
        "    print(\"Done submitting code.\")\n",
        "    print(\"Running test cases...\")\n",
        "\n",
        "    for index, testcode in enumerate(tests):\n",
        "        dispindex = index+1\n",
        "        summary = test_summary(dispindex, ntests, testcode)\n",
        "\n",
        "        try:\n",
        "            answer = run_test(testcode, get_lab_module())\n",
        "        except Exception:\n",
        "            show_exception(summary, testcode)\n",
        "            continue\n",
        "\n",
        "        correct, expected = server.send_answer(username, password, lab.__name__, testcode[0], type_encode(answer))\n",
        "        show_result(summary, testcode, correct, answer, expected, verbosity)\n",
        "        if correct: ncorrect += 1\n",
        "\n",
        "    response = server.status(username, password, lab.__name__)\n",
        "    print(response)\n",
        "\n",
        "\n",
        "def make_test_counter_decorator():\n",
        "    #tests = []\n",
        "    def make_test(getargs, testanswer, expected_val, name = None, type = 'FUNCTION'):\n",
        "        if name != None:\n",
        "            getargs_name = name\n",
        "        elif not callable(getargs):\n",
        "            getargs_name = \"_\".join(getargs[:-8].split('_')[:-1])\n",
        "            getargs = lambda: getargs\n",
        "        else:\n",
        "            getargs_name = \"_\".join(getargs.__name__[:-8].split('_')[:-1])\n",
        "\n",
        "        tests.append( ( getargs_name,\n",
        "                        getargs,\n",
        "                        testanswer,\n",
        "                        expected_val,\n",
        "                        getargs_name,\n",
        "                        type ) )\n",
        "\n",
        "    def get_tests():\n",
        "        return tests\n",
        "\n",
        "    return make_test, get_tests\n",
        "\n",
        "\n",
        "make_test, get_tests = make_test_counter_decorator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na4Dh04k0w8H"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUwtBcjB0ykF",
        "outputId": "0bead63e-bcf1-416b-acfa-a089d572ddb5"
      },
      "source": [
        "message = 'your trained neural-net on %s data must test with an accuracy of %1.3f'\n",
        "expected_accuracy = 1.0\n",
        "\n",
        "def neural_net_test_testanswer(val, original_val = None):\n",
        "    return abs(val - expected_accuracy) < 0.01\n",
        "\n",
        "network_maker_func = \"make_neural_net_two_layer\"\n",
        "network_min_size = 3\n",
        "max_iterations = 10000\n",
        "\n",
        "def neural_net_size_testanswer(val, original_val = None):\n",
        "    return val <= network_min_size\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [network_maker_func],\n",
        "          testanswer = neural_net_size_testanswer,\n",
        "          expected_val = \"your network must have <= %d neural units\"\\\n",
        "          %(network_min_size),\n",
        "          name = neural_net_size_tester\n",
        "          )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [network_maker_func,\n",
        "                             'and_data',\n",
        "                             'and_test_data',\n",
        "                             max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"AND\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [network_maker_func,\n",
        "                             'or_data',\n",
        "                             'or_test_data',\n",
        "                             max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"OR\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [network_maker_func,\n",
        "                             'neq_data',\n",
        "                             'neq_test_data',\n",
        "                             max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"XOR\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [network_maker_func,\n",
        "                             'equal_data',\n",
        "                             'equal_test_data',\n",
        "                             max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"EQUAL\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [network_maker_func,\n",
        "                             'diag_band_data',\n",
        "                             'diag_band_test_data',\n",
        "                             max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"diagonal_band\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "challenging_network_maker_func = \"make_neural_net_challenging\"\n",
        "challenging_network_min_size = 5\n",
        "\n",
        "def challenging_neural_net_size_testanswer(val, original_val = None):\n",
        "    return val <= challenging_network_min_size\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [challenging_network_maker_func],\n",
        "          testanswer = challenging_neural_net_size_testanswer,\n",
        "          expected_val = \"your network must have <= %d neural units\"\\\n",
        "          %(network_min_size),\n",
        "          name = neural_net_size_tester\n",
        "          )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [challenging_network_maker_func,\n",
        "                             'letter_l_data',\n",
        "                             'letter_l_test_data',\n",
        "                             max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"letter-l\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "manual_weights_network_maker_func = \"make_neural_net_with_weights\"\n",
        "manual_weights_network_max_iterations = 3000\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = lambda: [manual_weights_network_maker_func,\n",
        "                             'patch_data',\n",
        "                             'patch_test_data',\n",
        "                             manual_weights_network_max_iterations],\n",
        "          testanswer = neural_net_test_testanswer,\n",
        "          expected_val = message %(\"patchy\", expected_accuracy),\n",
        "          name = neural_net_tester\n",
        "          )\n",
        "\n",
        "republican_newspaper_vote_getargs = republican_newspaper_vote\n",
        "\n",
        "def republican_newspaper_vote_testanswer(val, original_val = None):\n",
        "    return ( val in ('no', 'No', 'nay', 'Nay', 'NO') )\n",
        "\n",
        "make_test(type = 'VALUE',\n",
        "          getargs = republican_newspaper_vote_getargs,\n",
        "          testanswer = republican_newspaper_vote_testanswer,\n",
        "          expected_val = \"no\",\n",
        "          name = republican_newspaper_vote_getargs\n",
        "          )\n",
        "\n",
        "def classifier_tester_1_getargs():\n",
        "    return [ 'boost_1796', 'house_1796' ]\n",
        "\n",
        "def classifier_tester_1_testanswer(val, original_val = None):\n",
        "    [0.077586206896551727, 0.077586206896551727, 0.034482758620689655,\n",
        "    0.051724137931034482, 0.025862068965517241, 0.025862068965517241, 0.0,\n",
        "    0.0086206896551724137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
        "    0.0, 0.0]\n",
        "    return (abs(val[0]-.077586 < 0.0001) and val[19] < 0.0001)\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = classifier_tester_1_getargs,\n",
        "          testanswer = classifier_tester_1_testanswer,\n",
        "          expected_val = \"A classifier that improves as much as would be expected by boosting, over 20 steps\",\n",
        "          name = classifier_tester\n",
        "          )\n",
        "\n",
        "def most_misclassified_boost_1796_getargs():\n",
        "    return [ 5 ]\n",
        "\n",
        "def most_misclassified_boost_1796_testanswer(val, original_val = None):\n",
        "    return ( len(val) == 5 and 'Dayton (New Jersey-98)' in val )\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = most_misclassified_boost_1796_getargs,\n",
        "          testanswer = most_misclassified_boost_1796_testanswer,\n",
        "          expected_val = \"Five hard-to-classify Congressmen including Dayton\",\n",
        "          name = most_misclassified_boost_1796\n",
        "          )\n",
        "\n",
        "\n",
        "republican_sunset_vote_getargs = republican_sunset_vote\n",
        "\n",
        "def republican_sunset_vote_testanswer(val, original_val = None):\n",
        "    return ( val in ('no', 'No', 'nay', 'Nay') )\n",
        "\n",
        "make_test(type = 'VALUE',\n",
        "          getargs = republican_sunset_vote_getargs,\n",
        "          testanswer = republican_sunset_vote_testanswer,\n",
        "          expected_val = \"'no'\",\n",
        "          name = republican_sunset_vote_getargs\n",
        "          )\n",
        "\n",
        "\n",
        "def most_misclassified_boost_getargs():\n",
        "    return [ 5 ]\n",
        "\n",
        "def most_misclassified_boost_testanswer(val, original_val = None):\n",
        "    return len(val) == 5\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = most_misclassified_boost_getargs,\n",
        "          testanswer = most_misclassified_boost_testanswer,\n",
        "          expected_val = \"Five hard-to-classify Senators\",\n",
        "          name = most_misclassified_boost\n",
        "          )\n",
        "\n",
        "\n",
        "def classifier_tester_2_getargs():\n",
        "    return [ 'boost', 'senate_people' ]\n",
        "\n",
        "def classifier_tester_2_testanswer(val, original_val = None):\n",
        "    if abs(val[0]-.0098) > .0001: return False\n",
        "\n",
        "    for x in range(10,len(val)):\n",
        "        if val[x] > .0001:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "make_test(type = 'FUNCTION',\n",
        "          getargs = classifier_tester_2_getargs,\n",
        "          testanswer = classifier_tester_2_testanswer,\n",
        "          expected_val = \"A classifier that improves as much as would be expected by boosting, over 20 steps\",\n",
        "          name = classifier_tester\n",
        "          )\n",
        "\n",
        "test_offline()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1/15 (<function neural_net_size_tester at 0x7fc0728e6e18>): Correct.\n",
            "Test 2/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 3/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 4/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 5/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 6/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 7/15 (<function neural_net_size_tester at 0x7fc0728e6e18>): Correct.\n",
            "Test 8/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 9/15 (<function neural_net_tester at 0x7fc07a69c2f0>): Correct.\n",
            "Test 10/15 (no): Correct.\n",
            "Test 11/15 (<function classifier_tester at 0x7fc07a9d89d8>): Correct.\n",
            "Test 12/15 (<function <lambda> at 0x7fc07acdfc80>): Correct.\n",
            "Test 13/15 (no): Correct.\n",
            "Test 14/15 (<function <lambda> at 0x7fc0728e6d90>): Correct.\n",
            "Test 15/15 (<function classifier_tester at 0x7fc07a9d89d8>): Correct.\n",
            "Passed 15 of 15 tests.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    }
  ]
}